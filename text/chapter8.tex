

\chapter{More Definite Clause Grammars}\label{CHAPTER8}

\Thischapter{130}{

\noindent
This chapter has two main goals:

\begin{enumerate}

\item{}To examine two important capabilities offered by DCG notation:
extra arguments and extra goals.

\item{}To discuss the status and limitations of DCGs.
\end{enumerate}

}




\section{Extra Arguments}\label{SEC.L8.EXTRA.ARGUMENTS}

In the previous chapter we introduced basic DCG notation. But DCGs
offer  more than we've seen so far.  For a start, DCGs allow us to
specify extra arguments.  Extra arguments can be used for many
purposes; we'll examine three.




\subsection*{Context free grammars with features}\label{SUBSEC.L8.GRAMMARS.FEATURES}


As a first example, let's see how extra arguments can be used to add
\LPNemph{features} to context-free grammars.

Here's the  DCG we worked with in the previous chapter:

\begin{LPNcodedisplay}
s --> np,vp.

np --> det,n.

vp --> v,np.
vp --> v.

det --> [the].
det --> [a].

n --> [woman].
n --> [man].

v --> [shoots].
\end{LPNcodedisplay}

Now, suppose we wanted to deal with sentences like ``She shoots him'',
and ``He shoots her''.  What should we do?  Well, obviously we should
add rules saying that ``he'', ``she'', ``him'', and ``her'' are
pronouns:
\begin{LPNcodedisplay}
pro --> [he].
pro --> [she].
pro --> [him].
pro --> [her].
\end{LPNcodedisplay}
Furthermore, we should add a rule saying that noun phrases can be
pronouns:
\begin{LPNcodedisplay}
np --> pro.
\end{LPNcodedisplay}

In this new DCG any good? Well, up to a point, it works. For example:
\begin{LPNcodedisplay}
?- s([she,shoots,him],[]).
yes
\end{LPNcodedisplay}

But there's an obvious problem.  The DCG will also accept a lot of
sentences that are clearly wrong, such as ``A woman shoots she'',
``Her shoots a man'', and ``Her shoots she'':
\begin{LPNcodedisplay}
?- s([a,woman,shoots,she],[]).
yes

?- s([her,shoots,a,man],[]).
yes

?- s([her,shoots,she],[]).
yes
\end{LPNcodedisplay}

That is, the grammar doesn't know that ``she'' and ``he'' are \LPNemph{subject} pronouns and cannot be used in \LPNemph{object} position;
thus ``A woman shoots she'' is bad because it violates this basic fact
about English.  Moreover, the grammar doesn't know that ``her'' and
``him'' are \LPNemph{object} pronouns and cannot be used in \LPNemph{subject} position; thus ``Her shoots a man'' is bad because it
violates this constraint.  As for ``Her shoots she'', this manages to
get both matters wrong at once.

Now, it's pretty obvious \LPNemph{what} we have to do to put this right:
we need to extend the DCG with information about which pronouns can
occur in subject position and which in object position.  The
interesting question: \LPNemph{how} exactly are we to do this?  First
let's look at a naive way of correcting this, namely adding new rules:

\begin{LPNcodedisplay}
s --> np_subject,vp.

np_subject --> det,n.
np_object  --> det,n.
np_subject --> pro_subject.
np_object  --> pro_object.

vp --> v,np_object.
vp --> v.

det --> [the].
det --> [a].

n --> [woman].
n --> [man].

pro_subject --> [he].
pro_subject --> [she].
pro_object --> [him].
pro_object --> [her].

v --> [shoots].
\end{LPNcodedisplay}


Now this solution ``works''. For example,
\begin{LPNcodedisplay}
?- s([her,shoots,she],[]).
no
\end{LPNcodedisplay}

But neither computer scientists nor linguists would consider this a
good solution.  The trouble is, a small addition to the lexicon has
led to quite a big change in the DCG. Let's face it: ``she'' and
``her'' (and ``he'' and ``him'') are the same in a lot of respects.
But to deal with the property in which they differ (namely, in which
position they can occur in sentences) we've had to make big changes to
the grammar: in particular, we've doubled the number of noun phrase
rules.  If we had to make further changes (for example, to cope with
plural noun phrases) things would get even worse.  What we really need
is a more delicate programming mechanism that allows us to cope with
such facts without being forced to add rules all the time.  And here's
where the extra arguments come into play.  Look at the following
grammar:

\begin{LPNcodedisplay}
s --> np(subject),vp.

np(_) --> det,n.
np(X) --> pro(X).

vp --> v,np(object).
vp --> v.

det --> [the].
det --> [a].

n --> [woman].
n --> [man].

pro(subject) --> [he].
pro(subject) --> [she].
pro(object) --> [him].
pro(object) --> [her].

v --> [shoots].
\end{LPNcodedisplay}




The key thing to note is that this new grammar contains only one new
noun phrase rule.  In fact, it is very similar to the first grammar
that we wrote, except that now the symbol "np" is associated with a
new argument, either "subject", "object", "_" or "X".  A
linguist would say that we've added \LPNterm{features} to distinguish
various kinds of noun phrase.  In particular, note the four rules for
the pronouns. Here we've used the extra argument to state which
pronouns can occur in subject position, and which can occur in object
position. Thus these rules are the most fundamental, for they give us
the basic facts about how these pronouns can be used.

So what do the other rules do? Well, intuitively, the rule
\begin{LPNcodedisplay}
np(X) --> pro(X).
\end{LPNcodedisplay}
uses the extra argument (the variable "X") to pass these basic facts
about pronouns up to noun phrases built out of them: because the
variable "X" is used as the extra argument for both the np and the
pronoun, Prolog unification will guarantee that they will be given the
same value. In particular, if the pronoun we use is ``she'' (in which
case "X=subject"), then the np will (through its extra argument
"X=subject") be marked as a subject np. On the other hand, if the
pronoun we use is ``her'' (in which case "X=object"), then the extra
argument for np will be marked "X=object" too. And this, of course, is
exactly the behaviour we want.

On the other hand, although noun phrases built using the rule
\begin{LPNcodedisplay}
np(_) --> det,n.
\end{LPNcodedisplay}
also have an extra argument, we've used the anonymous variable as its
value.  Essentially this means \LPNemph{can be either}, which is
correct, for expressions built using this rule (such as ``the man''
and ``a woman'') can be used in both subject and object position.

Now consider the rule
\begin{LPNcodedisplay}
vp --> v,np(object).
\end{LPNcodedisplay}
This says that to apply this rule we need to use a noun phrase whose
extra argument unifies with "object".  This can be \LPNemph{either}
noun phrases built from object pronouns \LPNemph{or} noun phrases such as
``the man'' and ``a woman'' which have the anonymous variable as the
value of the extra argument.  Crucially, pronouns marked has having
"subject" as the value of the extra argument \LPNemph{can't} be used
here: the atoms "object" and "subject" don't unify.  Note
that the rule
\begin{LPNcodedisplay}
s --> np(subject),vp.
\end{LPNcodedisplay}
works in an analogous fashion to prevent noun phrases made of object
pronouns from ending up in subject position.

This works.  You can check it out by posing the query:
\begin{LPNcodedisplay}
?- s(X,[]).
\end{LPNcodedisplay}
As you step through the responses, you'll see that only acceptable
English is generated.

But while the intuitive explanation just given is correct, what's
\LPNemph{really} going on?  The key thing to remember is that DCG
rules are just a convenient abbreviation.  For example, the
rule
\begin{LPNcodedisplay}
s --> np,vp.
\end{LPNcodedisplay}
is really syntactic sugar for
\begin{LPNcodedisplay}
s(A,B) :-
    np(A,C),
    vp(C,B).
\end{LPNcodedisplay}
That is, as we learned in the previous chapter, the DCG notation is a
way of hiding the two arguments responsible for the difference list
representation, so that we don't have to think about them.  We work
with the nice user-friendly notation, and Prolog translates it into
the clauses just given.

Ok, so we obviously need to ask what
\begin{LPNcodedisplay}
s --> np(subject),vp.
\end{LPNcodedisplay}
translates into. Here's the answer:
\begin{LPNcodedisplay}
s(A,B) :-
    np(subject,A,C),
    vp(C,B).
\end{LPNcodedisplay}

As should now be clear, the name ``extra argument'' is a good one: as
this translation makes clear, the "subject" symbol really
\LPNemph{is} just one more argument in an ordinary Prolog rule.
Similarly, our noun phrase DCG rules translate into
\begin{LPNcodedisplay}
np(A,B,C) :-
    det(B,D),
    n(D,C).
np(A,B,C) :-
    pro(A,B,C).
\end{LPNcodedisplay}
Note that both rules have \LPNemph{three} arguments.  The first, "A",
is the extra argument, and the last two are the ordinary, hidden DCG
arguments (the two hidden arguments are always the last two
arguments).

Incidentally, how do you think we would use the grammar to list the
grammatical noun phrases?  Well, if we had been working with the DCG
rule "np --> det,n" (that is, a rule with no extra arguments)
we would have made the query
\begin{LPNcodedisplay}
?- np(NP,[]).
\end{LPNcodedisplay}
So, in view of what we have just learned about extra arguments, it's
not too surprising that we need to pose the query
\begin{LPNcodedisplay}
?- np(X,NP,[]).
\end{LPNcodedisplay}
when working with our new DCG. And here's what the response would be:
\begin{LPNcodedisplay}
X = _2625
NP = [the,woman] ;

X = _2625
NP = [the,man] ;

X = _2625
NP = [a,woman] ;

X = _2625
NP = [a,man] ;

X = subject
NP = [he] ;

X = subject
NP = [she] ;

X = object
NP = [him] ;

X = object
NP = [her] ;

no
\end{LPNcodedisplay}




One final remark: don't be misled by this simplicity of our example
grammar.  Extra arguments can be used to cope with some complex
syntactic problems.  DCGs are no longer the state-of-the-art grammar
development tools they once were, but they're not toys either.  Once
you know about writing DCGs with extra arguments, you can write some
fairly sophisticated grammars.

\subsection*{Building parse trees}\label{SUBSEC.L8.PARSE.TREES}



So far, the programs we have discussed have been able to
\LPNemph{recognise}
grammatical structure (that is, they could correctly
answer yes  or  no when asked whether the input was a sentence,
a noun phrase, and so on) and to \LPNemph{generate} grammatical
output.  This is pleasant, but we would also like to be able to
\LPNemph{parse}.  That is, we would like our programs not only
to tell us \LPNemph{which} sentences are grammatical, but also to give
us an analysis of their structure.  In particular, we would like to
see the trees the grammar assigns to sentences.

Well, using only standard Prolog tools we can't actually draw nice
pictures of trees, but we \LPNemph{can} build data structures which
describe trees in a clear way.  For example, corresponding to the tree

\begin{quote}
  \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{s}}}
    { \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{np}}}
             {\pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{det}}}
                     {\TR{\texttt{a}}}
              \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{n}}}
                     {\TR{\texttt{woman}}}}
      \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{vp}}}
             {\pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{v}}}
                     {\TR{\texttt{shoots}}}
             }}
\end{quote}
%\includegraphics{parse2.eps}
we could have the following term:
\begin{LPNcodedisplay}
s(np(det(a),n(woman)),vp(v(shoots))).
\end{LPNcodedisplay}
Sure: it doesn't \LPNemph{look} as nice, but all the information in
the picture is there.  And, with the aid of a decent graphics package,
it would be easy to turn this term into a picture.

But how do we get DCGs to build such terms?  Actually, it's pretty
easy.  After all, in effect a DCG has to work out what the tree
structure is when recognising a sentence.  So we just need to find a
way of keeping track of the structure that the DCG finds.  We do this
by adding extra arguments.  Here's how:
\begin{LPNcodedisplay}
s(s(NP,VP)) --> np(NP),vp(VP).

np(np(DET,N)) --> det(DET),n(N).

vp(vp(V,NP)) --> v(V),np(NP).
vp(vp(V))    --> v(V).

det(det(the)) --> [the].
det(det(a))   --> [a].

n(n(woman)) --> [woman].
n(n(man))   --> [man].

v(v(shoots)) --> [shoots].
\end{LPNcodedisplay}


What's going on here? Essentially we are building the parse trees for
the syntactic categories on the left hand side of the rules out of the
parse trees for the syntactic categories on the right hand side of the
rules. Consider the rule "vp(vp(V,NP)) --> v(V),np(NP)".  When we make
a query using this DCG, the "V" in "v(V)" and the "NP" in "np(NP)"
will be instantiated to terms representing parse trees. For example,
perhaps "V" will be instantiated to
\begin{LPNcodedisplay}
v(shoots)
\end{LPNcodedisplay}
and "NP" will be instantiated to
\begin{LPNcodedisplay}
np(det(a),n(woman)).
\end{LPNcodedisplay}
What is the term corresponding to a vp made out of these
two structures? Obviously it should be this:
\begin{LPNcodedisplay}
vp(v(shoots),np(det(a),n(woman))).
\end{LPNcodedisplay}
And this is precisely what the extra argument "vp(V,NP)" given in the
rule "vp(vp(V,NP)) -->" "v(V),np(NP)" returns to us:  a term
whose functor is "vp", and whose first and second arguments are the
values of "V" and "NP" respectively. To put it informally: it plugs
the "V" and the "NP" terms together under a "vp" functor.

To parse the sentence ``A woman shoots'' we pose the query:
\begin{LPNcodedisplay}
?- s(T,[a,woman,shoots],[]).
\end{LPNcodedisplay}
That is, we ask for the extra argument "T" to be instantiated
to a parse tree for the sentence.  And we get:
\begin{LPNcodedisplay}
T = s(np(det(a),n(woman)),vp(v(shoots)))
yes
\end{LPNcodedisplay}

Furthermore, we can generate all parse trees by making the following
query:
\begin{LPNcodedisplay}
?- s(T,S,[]).
\end{LPNcodedisplay}
The first three responses are:
\begin{LPNcodedisplay}
T = s(np(det(the),n(woman)),
      vp(v(shoots),np(det(the),n(woman))))
S = [the,woman,shoots,the,woman] ;

T = s(np(det(the),n(woman)),
      vp(v(shoots),np(det(the),n(man))))
S = [the,woman,shoots,the,man] ;

T = s(np(det(the),n(woman)),
      vp(v(shoots),np(det(a),n(woman))))
S = [the,woman,shoots,a,woman]
\end{LPNcodedisplay}
In short, we have just seen an elegant (and useful) example of how to
build structure using unification.

Extra arguments can also be used to build \LPNterm{semantic
representations}.  Now, we did not say anything about what the words
in our little DCG mean.  In fact, nowadays a lot is known about the
semantics of natural languages, and it is surprisingly easy to build
semantic representations which partially capture the meaning of
sentences or even entire discourses.  Such representations are usually
expressions of some formal language (for example first-order logic,
discourse representation structures, or a database query language) and
they are usually built up \LPNterm{compositio\-nally}.  That is, the
meaning of each word is expressed in the formal language; this meaning
is given as an extra argument in the DCG entries for the individual
words.  Then, for each rule in the grammar, an extra argument shows
how to combine the meaning of the two subcomponents.  For example, to
the rule "s --> np, vp" we would add an extra argument stating how to
combine the "np" meaning and the "vp" meaning to form the "s" meaning.
Although somewhat more complex, the semantic construction process is
quite like the way we built up the parse tree for the sentence from
the parse tree of its subparts.%
\footnote{For a detailed account of how to do this, see
\textit{Representation and Inference for Natural Language: A First
Course in Computational Semantics}, Patrick Blackburn and Johan Bos,
CSLI Publications, 2005.}


\subsection*{Beyond context free languages}\label{SUBSEC.L8.NON.CONTEXT.FREE}



In the previous chapter we introduced DCGs as a useful Prolog tool for
representing and working with context free grammars. Now, this is
certainly a good way of thinking about DCGs, but it's not the whole
story. For the fact of the matter is: DCGs can deal with a lot more
than just context free languages. The extra arguments we have been
discussing (and indeed, the extra goals we shall introduce shortly)
give us the tools for coping with any computable language
whatsoever. We shall illustrate this by presenting a simple DCG for the
formal language $a^nb^nc^n \backslash \{ \epsilon \}$.

The formal language $a^nb^nc^n \backslash \{ \epsilon \}$  consists
of all non-null strings made up of "a"s, "b"s, and "c"s which consist
of an unbroken block of "a"s, followed by an unbroken block of "b"s,
followed by an unbroken block of "c"s, all three blocks having the
same length. For example, "abc", and "aabbcc" and "aaabbbccc" all
belong to
$a^nb^nc^n \backslash \{ \epsilon \}$.

The interesting thing about this language is that it is \LPNemph{not}
context free. Try whatever you like, you will not succeed in writing a
context free grammar that generates precisely these strings. Proving
this would take us too far afield, but the proof is not particularly
difficult, and you can find it in many books on formal language
theory.

 On the other hand, as we shall now see, it is very easy to write a
DCG that generates this language. Just as we did in the previous
chapter, we shall represent strings as lists; for example, the string
"abc" will be represented using the list "[a,b,c]". Given this
convention, here's the DCG we need:
\begin{LPNcodedisplay}
s(Count) --> ablock(Count),bblock(Count),cblock(Count).

ablock(0) --> [].
ablock(succ(Count)) --> [a],ablock(Count).

bblock(0) --> [].
bblock(succ(Count)) --> [b],bblock(Count).

cblock(0) --> [].
cblock(succ(Count)) --> [c],cblock(Count).
\end{LPNcodedisplay}


The idea underlying this DCG is fairly simple: we use an extra
argument to keep track of the length of the blocks. The "s" rule
simply says that we want a block of "a"s followed by a block of
"b"s followed by block of "c"s, and all three blocks are to
have the same length, namely "Count".

What should the values of "Count" be? The obvious answer is: "1", "2",
"3", "4", and so on.  But as yet we don't know how to mix DCGs and
arithmetic, so this isn't very helpful. Fortunately, as this DCG
shows, there's an easier (and more elegant) way. Represent the
number 0 by "0", the number 1 by "succ(0)", the number 2 by
"succ(succ(0))", the number 3 by "succ(succ(succ(0)))", and so on,
just as we did it in Chapter~3 (as we said in Chapter~3, you can read
"succ" as ``successor of'').  This choice of notation
enables us to count using unification.

And this is precisely what our new  DCG does.
For example, suppose we pose the following query:
\begin{LPNcodedisplay}
?- s(Count,L,[]).
\end{LPNcodedisplay}
which asks Prolog to generate the lists "L" of symbols that
belong to this language, and to give the value of "Count" needed
to produce each item. Then the first four responses are:
\begin{LPNcodedisplay}
Count = 0
L = [] ;

Count = succ(0)
L = [a, b, c] ;

Count = succ(succ(0))
L = [a, a, b, b, c, c] ;

Count = succ(succ(succ(0)))
L = [a, a, a, b, b, b, c, c, c]
\end{LPNcodedisplay}

The value of "Count" clearly corresponds to the
length of the blocks.

 So: DCGs are not just a tool for working with context free
grammars. They are strictly more powerful than that, and (as we've
just seen) part of the extra power comes from the use of extra
arguments.

\section{Extra Goals}\label{SEC.L8.TESTS}



Any DCG rule is really syntactic sugar for an ordinary Prolog rule.
So it's not really too surprising that we're allowed to make use of
extra arguments.  Similarly, it shouldn't come as too much of a
surprise that we can call any Prolog predicate whatsoever from the
right hand side of a DCG rule.

The DCG of the previous section can, for example, be adapted to work
with Prolog numbers (instead of the successor representation of
numbers) by using calls to Prolog's built-in arithmetic
functionality. We simply count how many "a"s, "b"s, and "c"s have been
generated. Here's the code:
\begin{LPNcodedisplay}
s --> ablock(Count),bblock(Count),cblock(Count).

ablock(0) --> [].
ablock(NewCount) --> [a],ablock(Count),
                     {NewCount is Count + 1}.

bblock(0) --> [].
bblock(NewCount) --> [b],bblock(Count),
                     {NewCount is Count + 1}.

cblock(0) --> [].
cblock(NewCount) --> [c],cblock(Count),
                     {NewCount is Count + 1}.
\end{LPNcodedisplay}

As this example suggests, extra goals can be written (anywhere) on the
right side of a DCG rule, but must be placed between curly
brackets. When Prolog encounters such curly brackets while translating
a DCG into its internal representation, it just takes the extra goals
specified between the curly brackets over into the translation. So,
the second rule for the non-terminal "ablock" above would be
translated as follows:
\begin{LPNcodedisplay}
ablock(NewCount,A,B):-
   'C'(A, a, C),
   ablock(Count, C, B),
   NewCount is Count + 1.
\end{LPNcodedisplay}


Incidentally, if you play around with this DCG, you will find that
there are actually some problems with it. In contrast to the one that
we saw in the last section, this new version only works correctly when
used in the recognition mode. If you try to generate with it, it will
at some point enter an infinite loop. We won't bother to fix this
problem here (apart from anything else, we find the earlier "succ"
based approach more elegant).

The possibility of adding arbitrary Prolog goals to the right hand
side of DCG rules, makes DCGs very powerful (it means that we can do
anything that we can do in plain Prolog). In general, however, this
capability is not used much, which tends to suggest that the basic DCG
notation is well designed.  There is, however, one classic application
for extra goals in computational linguistics: with the help of extra
goals, we can neatly separate grammar rules and lexical
information. Let's see how.

\subsection*{Separating rules and lexicon}\label{SUBSEC.L8.SEP.RULE.LEX}


We are going to separate rules and lexicon. That is, we are going to
eliminate all mention of individual words in our DCGs and instead
record all the information about individual words separately in a
lexicon.  To see what is meant by this, let's return to our basic
grammar:
\begin{LPNcodedisplay}
np --> det,n.

vp --> v,np.
vp --> v.

det --> [the].
det --> [a].

n --> [woman].
n --> [man].

v --> [shoots].
\end{LPNcodedisplay}
We are now going to write a DCG that generates exactly
the same language, but in which no rule mentions any individual
word. All the information about individual words will be recorded
separately.


Here is an example of a (very simple) lexicon. Lexical entries are
encoded by using a predicate "lex/2" whose first argument is a
word, and whose second argument is a syntactic category.
\begin{LPNcodedisplay}
lex(the,det).
lex(a,det).
lex(woman,n).
lex(man,n).
lex(shoots,v).
\end{LPNcodedisplay}

And here is a simple grammar that could go with this lexicon.  In
essence it's the same as the previous one.  In fact, the only rules
that have changed are those that mentioned specific words, that is,
the "det", "n", and "v" rules.
\begin{LPNcodedisplay}
np --> det,n.

vp --> v,np.
vp --> v.

det --> [Word],{lex(Word,det)}.
n --> [Word],{lex(Word,n)}.
v --> [Word],{lex(Word,v)}.
\end{LPNcodedisplay}

Consider the new "det" rule. This rule part says ``a "det"
can consist of a list containing a single element "Word"'' (note
that "Word" is a variable). Then the extra goal adds the crucial
stipulation: ``so long as "Word" unifies  with something that is
listed in the lexicon as a determiner''.  With our present lexicon,
this means that "Word" must be matched either with the word ``a''
or ``the''.  So this single rule replaces the two previous DCG rules
for "det".

This explains the ``how'' of separating rules from lexicon, but it
doesn't explain the ``why''. Is it really so important? Is this new
way of writing DCGs really that much better?

The answer is an unequivocal yes! It's \LPNemph{much} better, and for
at least two reasons.

The first reason is theoretical. Arguably rules should not mention
specific lexical items. The purpose of rules is to list
\LPNemph{general} syntactic facts, such as the fact that sentence can
be made up of a noun phrase followed by a verb phrase. The rules for
"s", "np", and "vp" describe such general syntactic facts, but the old
rules for "det", "n", and "v" don't. Instead, the old rules simply
list particular facts: that ``a'' is a determiner, that ``the'' is a
determiner, and so on.  From theoretical perspective it is much neater
to have a single rule that says ``anything is a determiner (or a noun,
or a verb, or any other grammatical category) if it is listed as such
in the lexicon''. And this, of course, is precisely what our new DCG
rules say.

The second reason is more practical. One of the key lessons
computational linguists have learnt over the last twenty or so years
is that the lexicon is by far the most interesting, important (and
expensive!) repository of linguistic knowledge. Bluntly, if you want
to get to grips with natural language from a computational perspective,
you need to know a lot of words, and you need to know a lot about them.

 Now, our little lexicon, with its simple two-place "lex" entries, is
a toy. But a real lexicon is (most emphatically!) not. A real lexicon
is likely to be very large (it may contain hundreds of thousands of
words) and moreover, the information associated with each word is
likely to be very rich.  Our "lex" entries give only the syntactical
category of each word, but a real lexicon will give much more, such as
information about its phonological, morphological, semantic, and
pragmatic properties.

Because real lexicons are big and complex, from a software engineering
perspective it is best to write simple grammars that have a simple,
well-defined way, of pulling out the information they need from vast
lexicons. That is, grammars should be thought of as separate entities
which can access the information contained in lexicons. We can then
use specialised mechanisms for efficiently storing the lexicon and
retrieving data from it.

Our new DCG rules, though simple, illustrate the basic idea. The new
rules really do just list general syntactic facts, and the extra goals
act as an interface to our lexicon that lets the rules find exactly
the information they need. Furthermore, we now take advantage of
Prolog's
\LPNterm{first argument indexing} which makes looking up a word in the
lexicon more efficient. First argument indexing is a technique for
making Prolog's knowledge base access more efficient. If in the query
the first argument is instantiated it allows Prolog to ignore all
clauses where the first argument's functor and arity is
different. This means, for example, that we can get all the possible
categories of "man" immediately without having to even look at the
lexicon entries for all the other hundreds or thousands of words that
we might have in our lexicon.

\section{Concluding Remarks}\label{SEC.L8.CONCLUDING.REMARKS}



We now have a fairly useful picture of what DCGs are and what they can
do for us.  To conclude, let's think about them from a somewhat higher
level, from both a formal and a linguistic perspective.

First the formal remarks.  For the most part, we have presented DCGs
as a simple tool for encoding context free grammars (or context free
grammars enriched with features such as \LPNemph{subject} and
\LPNemph{object}).  But DCGs go beyond this.  We saw that it was
possible to write a DCG that generated a language that was not context
free.  In fact, \LPNemph{any program whatsoever} can be written in DCG
notation.  That is, DCGs are a full-fledged programming language in
their own right (they are Turing-complete, to use the proper
terminology).  And although DCGs are usually associated with
linguistic applications, they can be useful for other purposes.

How good are DCGs from a linguistic perspective?  Well, mixed.  At
one stage (in the early 1980s) they were pretty much state of the art.
They made it possible to code complex grammars in a clear way, and to
explore the interplay of syntactic and semantic ideas.  Certainly any
history of parsing in computational linguistics would give DCGs an
honourable mention.

Nonetheless, DCGs have drawbacks.  For a start, their tendency to loop
when the goal ordering is wrong (we saw an example in the previous
chapter when we added a left-recursive rule for conjunctions) is
annoying; we \LPNemph{don't} want to think about such issues when
writing serious grammars. Furthermore, while the ability to add extra
arguments is useful, if we need to use lots of them (and for big
grammars we will) it is a rather clumsy mechanism.

It is important to notice, however, that these problems come up
because of the way Prolog interprets DCG rules. They are not inherent
to the DCG notation. Any of you who have studied parsing algorithms
probably know that all top-down parsers loop on left-recursive
grammars. So, it is not surprising that Prolog, which interprets DCGs
in a top-down fashion, loops on the left-recursive grammar rule
"s --> s conj s". If we used a different strategy to interpret DCGs, for
example a bottom-up strategy, we would not run into the same
problem. Similarly, if we didn't use Prolog's built-in interpretation
of DCGs, we could use the extra arguments for a more sophisticated
specification of features, one that would facilitate the use of large
feature structures.

Summing up, nowadays DCGs are probably best viewed as a nice notation
for defining context free grammars enhanced with some features, a
notation that (ignoring left-recursion) doubles as a
parser/recogniser.  That is, they are best viewed as a convenient tool
for testing new grammatical ideas, or for implementing reasonably
complex grammars for particular applications.  DCGs are no longer
state of the art, but they are useful.  Even if you have never
programmed before, simply by using what you have learned so far you
are ready to start experimenting with reasonably sophisticated grammar
writing.  With a conventional programming language (such as C++ or
Java) it simply wouldn't be possible to reach this stage so soon.
Things would be easier in functional languages (such as Lisp, Caml, or
Haskell), but even so, it is doubtful whether beginners could do so
much so early.

\section{Exercises}\label{SEC.L8.EXERCISES}

\begin{LPNexercise}{L8.EX1} Here's our basic DCG:
\begin{LPNcodedisplay}
s --> np,vp.

np --> det,n.

vp --> v,np.
vp --> v.

det --> [the].
det --> [a].

n --> [woman].
n --> [man].
n --> [apple].
n --> [pear].

v --> [eats].
\end{LPNcodedisplay}


Suppose we add the noun ``men'' (which is plural) and the verb
``know''.  Then we would want a DCG which says that ``The men eat''
is ok, ``The man eats'' is ok, ``The men eats'' is not ok, and
``The man eat'' is not ok. Change the DCG so that it correctly
handles these sentences. Use an extra argument to cope with the
singular/plural distinction.
\end{LPNexercise}



\begin{LPNexercise}{L8.EX2}
In the text, we only gave examples of DCG rules
with one extra argument, but in fact you can add as many extra
arguments as you like. Here's a DCG rule
with three extra arguments:
\begin{LPNcodedisplay}
kanga(V,R,Q) --> roo(V,R),jumps(Q,Q),{marsupial(V,R,Q)}.
\end{LPNcodedisplay}
Translate it into the form Prolog uses.
\end{LPNexercise}

\section{Practical Session}\label{SEC.L8.PRAXIS}



The purpose of Practical Session 8 is to help you get familiar with
DCGs that make use of additional arguments and goals.

First some keyboard exercises:
\begin{enumerate}
\item{}Trace some examples using the DCG which uses extra arguments to
handle the subject/object distinction, the DCG which produces parses, and
the DCG which uses extra goals to separate lexicon and rules. Make sure
you fully understand the way all three DCGs work.

\item{}Carry out traces on the DCG for $a^nb^nc^n$ given in the text
(the one that gave the "Count" variable the values "0", "succ(0)",
"succ(succ(0))", and so on). Try cases where the three blocks of "a"s,
"b"s, and "c"s are indeed of the same length as well as queries where
this is not the case.
\end{enumerate}

Now for some programming. We suggest the following mini-project, which
draws on all you have learned so far. Incidentally, in the Practical
Session at the end of Chapter~12 we will be asking to extend this work
even further, so do take this project seriously.
\begin{enumerate}
\item{}First, bring together all the things we have learned about DCGs
for English into one DCG. In particular, in the text we saw how to use
extra arguments to deal with the subject/object distinction, and in
the exercises you were asked to use additional arguments to deal with
the singular/plural distinction. Write a DCG which handles
both. Moreover, write the DCG in such a way that it will produce parse
trees, and makes use of a separate lexicon.
\item{}Once you have done this, extend the DCG so that noun phrases can
be modified by adjectives and simple prepositional phrases (that is,
it should be able to handle noun phrases such as ``the small
frightened woman on the table'' or ``the big fat cow under the
shower''). Then, further extend it so that the distinction between
first, second, and third person pronouns is correctly handled (both in
subject and object form).
\end{enumerate}
