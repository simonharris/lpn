
\chapter{Definite Clause Grammars}\label{CHAPTER7}

\Thischapter{145}{

\noindent
This chapter has two main goals:

\begin{enumerate}
\item{}To introduce context-free grammars (CFGs) and some related
concepts.

\item{}To introduce definite clause grammars (DCGs), a built-in Prolog
mechanism for working with context-free grammars (and other kinds of
grammar too).
\end{enumerate}

}




\section{Context-Free Grammars}\label{SEC.L7.CFG}

Prolog has been used for many purposes, but its inventor, Alain
Colmerauer, was interested in computational linguistics, and this
remains a classic application for the language.  Moreover, Prolog
offers a number of tools which make life easier for computational
linguists, and we are now going to start learning about one of the
most useful of these: \LPNterm{definite clause grammars}, or DCGs as
they are usually called.

 DCGs are a special notation for defining \LPNterm{grammars}.  So,
before we go any further, we'd better learn what a grammar is.  We
shall do so by discussing \LPNterm{context-free grammars} (or CFGs).
The basic idea of context-free grammars is simple to understand, but
don't be fooled into thinking that CFGs are toys.  They're not.  While
CFGs aren't powerful enough to cope with the syntactic structure of
all natural languages (that is, the kind of languages that human
beings use), they can certainly handle most aspects of the syntax of
many natural languages (for example, English and French) in a
reasonably natural way.

 So what is a context-free grammar?  In essence, a finite
collection of rules which tell us that certain sentences are
grammatical (that is, syntactically correct) and what their
grammatical structure actually is.  Here's a simple context-free
grammar for a small fragment of English:


\begin{center}\begin{tabular}{l}
S $\rightarrow$ NP VP\\
NP $\rightarrow$ DET N\\
NP $\rightarrow$ V NP\\
VP $\rightarrow$ V\\
DET $\rightarrow$ ``a''\\
DET $\rightarrow$ \LPNemph{the}\\
N $\rightarrow$ \LPNemph{woman}\\
N $\rightarrow$ \LPNemph{man}\\
N $\rightarrow$ \LPNemph{shoots}
\end{tabular}\end{center}

What are the ingredients of this little grammar?  Well, first note
that it contains three types of symbol.  There's ``$\rightarrow$", which is used
to define the rules.  Then there are the symbols written like this:
"S", ``NP", ``VP", ``DET", ``N", ``V". These symbols are called
\LPNterm{non-terminal symbols}; we'll soon learn why.  Each of these
symbols has a traditional meaning in linguistics: ``S" is short for
sentence, ``NP" is short for noun phrase, ``VP" is short for verb
phrase, and ``DET" is short for determiner.  That is, each of these
symbols is shorthand for a grammatical category.  Finally there are
the symbols in lower case: \LPNemph{a, the, woman, man}, and
\LPNemph{shoots}.  These are \LPNterm{terminal symbols}, though a
computer scientist might call them the alphabet, and linguists might
call them lexical items.  We'll usually just call them words.

This grammar contains nine \LPNterm{context-free rules}.  A context
free rule consists of a single non-terminal symbol, followed by ``$\rightarrow$",
followed by a finite sequence made up of terminal and/or non-terminal
symbols.  All nine items listed above have this form, so they are all
legitimate context-free rules. What do these rules mean?  They tell us
how different grammatical categories can be built up.  Read ``$\rightarrow$" as
``can consist of'', or ``can be built out of''.''  For
example, the first rule tells us that a sentence can consist of a noun
phrase followed by a verb phrase.  The third rule tells us that a verb
phrase can consist of a verb followed by a noun phrase, while the
fourth rule tells us that there is another way to build a verb phrase:
simply use a verb.  The last five rules tell us that ``a'' and
``the'' are determiners, that ```man'' and ``woman''
are nouns, and that ``shoots'' is a verb.

Now consider the string of words ``a woman shoots a man''.  Is
this grammatical according to our little grammar?  And if it is, what
structure does it have?  The following tree answers both questions:

\begin{quote}
  \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{S}}}
    { \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{NP}}}
             {\pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{DET}}}
                     {\TR{\texttt{a}}}
              \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{N}}}
                     {\TR{\texttt{woman}}}}
      \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{VP}}}
             {\pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{V}}}
                     {\TR{\texttt{shoots}}}
              \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{NP}}}
                     {\pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{DET}}}
                             {\TR{\texttt{a}}}
                      \pstree[levelsep=10mm,nodesep=2mm]{\TR{\texttt{N}}}
                             {\TR{\texttt{man}}}}}}
\end{quote}

%\includegraphics{parse.eps}


Right at the top we have a node marked ``S". This node has two
daughters, one marked ``NP", and one marked ``VP". Note that
this part of the diagram agrees with the first rule of the grammar,
which says that an ``S" can be built out of an ``NP" and a
``VP". (A linguist would say that this part of the tree is
\LPNterm{licensed} by the first rule.)  In fact, as you can see,
\LPNemph{every} part of the tree is licensed by one of our rules.  For
example, the two nodes marked ``NP" are licensed by the rule that
says that an ``NP" can consist of a ``DET" followed by an
"N". And, right at the bottom of the diagram, all the words in
\LPNemph{a woman shoots a man} are licensed by a rule. Incidentally, note
that the terminal symbols only decorate the nodes right at the bottom
of the tree (the terminal nodes) while non-terminal symbols only
decorate nodes that are higher up in the tree (the non-terminal
nodes).

\clearpage
Such a tree is called a \LPNterm{parse tree}. Parse trees are important
because they give us two kinds of information.  Firstly, they give us
information about \LPNterm{strings}. Secondly, they give us information
about \LPNterm{structure}. This is an important distinction to grasp,
so let's have a closer look, and learn some important terminology
while we are doing so.

First, if we are given a string of words, and a grammar, and it turns
out that we \LPNemph{can} build a parse tree like the one above (that is, a
tree that has ``S" at the top node, and every node in the tree is
licensed by the grammar, and the string of words we were given is
listed in the correct order along the terminal nodes) then we say that
the string is \LPNterm{gramma\-tical} (according to the given grammar).  For
example, the string \LPNemph{a woman shoots a man} is grammatical according
to our little grammar (and indeed, any reasonable grammar of English
would classify it as grammatical). On the other hand, if there isn't
any such tree, the string is ungrammatical (according to the
given grammar). For example, the string \LPNemph{woman a woman man a
shoots} is ungrammatical according to our little grammar (and any
reasonable grammar of English would classify it as ungrammatical).
The \LPNterm{language generated by a grammar} consists of all the strings
that the grammar classifies as grammatical.  For example, \LPNemph{a woman
shoots a man} also belongs to the language generated by our little
grammar, and so does \LPNemph{a man shoots the woman}. A context-free
\LPNterm{recogniser} is a program which correctly tells us whether or not a
string belongs to the language generated by a context-free grammar.
To put it another way, a recogniser is a program that correctly classifies strings
as grammatical or ungrammatical (relative to some grammar).

But often, in both linguistics and computer science, we are not merely
interested in whether a string is grammatical or not, we also want to
know \LPNemph{why} it is grammatical.  More precisely, we often want
to know what its structure is, and this is exactly the information a
parse tree gives us. For example, the above parse tree shows us how
the words in \LPNemph{a woman shoots a man} fit together, piece by
piece, to form the sentence.  This kind of information would be
important if we were using this sentence in some application and
needed to say what it actually meant (that is, if we wanted to do
semantics). A context-free \LPNterm{parser} is a program which
correctly decides whether a string belongs to the language generated
by a context-free grammar \LPNemph{and also tells us what its
structure is}.  That is, whereas a recogniser merely says ``Yes,
grammatical'' or ``No, ungrammatical'' to each string, a parser actually
builds the associated parse tree and gives it to us.

\clearpage
It remains to explain one final concept, namely what a \LPNterm{context-
free language} is. (Don't get confused: we've told you what a context-
free \LPNemph{grammar} is, but not what a context-free
\LPNemph{language} is.)  Quite simply, a context-free language is a
language that can be generated by a context-free grammar. Some
languages are context-free, and some are not. For example, it seems
plausible that English is a context-free language. That is, it is
probably possible to write a context-free grammar that generates all
(and only) the sentences that native speakers find acceptable. On the
other hand, some dialects of Swiss-German are \LPNemph{not} context-
free. It can be proved mathematically that no context-free grammar can
generate all (and only) the sentences that native speakers of
Swiss-German find acceptable.%
\footnote{``Evidence against the context-freeness of natural language'',
Stuart M. Shieber,
\textit{Linguistics and Philosophy}, 8:333--343, 1985.}
So if you wanted to write a grammar for
such dialects, you would have to employ additional grammatical
mechanisms, not merely context-free rules.




\subsection*{CFG recognition using append}\label{SUBSEC.L7.NAIVE.IMPL}



That's the theory, but how do we work with context-free grammars in
Prolog?  To make things concrete: suppose we are given a context-free
grammar. How can we write a recogniser for it?  And how can we write a
parser for it? In this chapter we'll look at the first question in
detail.  We'll first show how (rather naive) recognisers can be
written in Prolog, and then show how more sophisticated recognisers
can be written with the help of difference lists.  This discussion
will lead us to definite clause grammars, Prolog's built-in grammar
tool.  In the following chapter we'll look at definite clause grammars
in more detail, and learn (among other things) how to use them to
define parsers.

So: given a context-free grammar, how do we define a recogniser in
Prolog?  In fact, Prolog offers a very direct answer to this question:
we can simply write down Prolog clauses that correspond, in an obvious
way, to the grammar rules.  That is, we can simply turn the grammar
into Prolog.

Here's a simple (though as we shall learn, inefficient) way of doing
this.  We shall use lists to represent strings.  For example, we shall
use the list ``[a,woman,shoots,a,man]" to represent the string
``a woman shoots a man''.  Now, we have already said that the
``$\rightarrow$" symbol used in context-free grammars means ``can consist
of'' or ``{can be built out of}'', and this idea is easily
modelled using lists.  For example, the Prolog rule ``s --$>$ np vp" can be
thought of as saying: a list of words is an ``s" list if it is the
result of concatenating an ``np" list with a ``vp" list.  As we know how
to concatenate lists in Prolog (we can use ``append/3"), it should be
easy to turn these kinds of rules into Prolog.  And what about the
rules that tell us about individual words?  Even easier: we can simply
view ``N $\rightarrow$ \LPNemph{woman}'' as saying that the list ``[woman]" is an
``n" list.

If we turn these ideas into Prolog, this is what we get:
\begin{LPNcodedisplay}
s(Z):- np(X), vp(Y), append(X,Y,Z).

np(Z):- det(X), n(Y), append(X,Y,Z).

vp(Z):- v(X), np(Y), append(X,Y,Z).

vp(Z):- v(Z).

det([the]).
det([a]).

n([woman]).
n([man]).

v([shoots]).
\end{LPNcodedisplay}


The correspondence between the CFG rules and the Prolog code should be
clear.  And to use this program as a recogniser, we simply pose the
obvious queries.  For example:
\begin{LPNcodedisplay}
?- s([a,woman,shoots,a,man]).
yes
\end{LPNcodedisplay}

In fact, because this is a simple declarative Prolog program, we can
do more than this: we can also \LPNterm{generate} all the sentences this
grammar produces.  Our little grammar generates 20 sentences.
Here are the first five:
\begin{LPNcodedisplay}
?- s(X).

X = [the,woman,shoots,the,woman] ;

X = [the,woman,shoots,the,man] ;

X = [the,woman,shoots,a,woman] ;

X = [the,woman,shoots,a,man] ;

X = [the,woman,shoots]
\end{LPNcodedisplay}

Moreover, we're not restricted to posing questions about sentences: we
can ask about other grammatical categories. For example:
\begin{LPNcodedisplay}
?- np([a,woman]).
yes
\end{LPNcodedisplay}

And we can generate noun phrases with the following query.
\begin{LPNcodedisplay}
?- np(X).
\end{LPNcodedisplay}


Now this is rather nice.  We have a simple, easy to understand program
which corresponds with our CFG in an obvious way.  Moreover, if we
added more rules to our CFG, it would be easy to alter the program to
cope with the new rules.

But there is a problem: the program doesn't use the input sentence to
guide the search. Make a trace for the query ``s([a,man,shoots])"
and you will see that the program chooses
noun phrases and verb
phrases and only afterwards checks whether these can be combined to
form the sentence ``[a,man,shoots]". For example, Prolog will find that
"[the,woman]" is a noun phrase and ``[shoots,the,woman]" a
verb phrase and only then will it check whether concatenating these
lists happens to yield ``[a,man,shoots]", which of course
it won't. So, Prolog starts to backtrack, and the next thing it will try
is whether concatenating the noun phrase ``[the,woman]" and the
verb phrase ``[shoots,the,man]" happens to yield
"[a,man,shoots]", another non-starter.
It will go on like this until it (finally)
produces the noun phrase ``[a,man]" and the verb phrase
"[shoots]". The problem is that the goals
"np(X)" and ``vp(Y)" are called with uninstantiated variables
as arguments.

So, how about changing the rules in such a way that ``append"
becomes the first goal:
\begin{LPNcodedisplay}
s(Z):- append(X,Y,Z), np(X), vp(Y).

np(Z):- append(X,Y,Z), det(X), n(Y).

vp(Z):- append(X,Y,Z), v(X), np(Y).

vp(Z):- v(Z).

det([the]).
det([a]).

n([woman]).
n([man]).

v([shoots]).
\end{LPNcodedisplay}

Here we first use ``append/3" to split up the input list. This
instantiates the variables ``X" and ``Y", so that the other goals are
all called with instantiated arguments. However, this program is still
not very appealing: it uses ``append/3" a lot and, even worse, it uses
"append/3" with uninstantiated variables in the first two
arguments. We saw in the previous chapter that this is a source of
inefficiency. And indeed, the performance of this recogniser is very
bad.  It is revealing to trace through what actually happens when this
program analyses a sentence such as \LPNemph{a woman shoots a man}.
As you will see, relatively few of the steps are devoted to the real
task of recognising the sentences: most are devoted to using
"append/3" to decompose lists.  This isn't much of a problem for our
little grammar, but it certainly would be if we were working with a
more realistic grammar capable of generating a large number of
sentences. We need to do something about this.

\subsection*{CFG recognition using difference lists}\label{SUBSEC.L7.DIFF.STRUCTURES}



A more efficient implementation can be obtained by making use of
\LPNemph{difference lists}. This is a sophisticated (and, once you've
grasped it, beautiful) Prolog technique that can be used for a
variety of purposes.


The key idea underlying difference lists is to represent the
information about grammatical categories not as a single list, but as
the difference between two lists.  For example, instead of
representing \LPNemph{a woman shoots a man} as
"[a,woman,shoots,a,man]" we can represent it as the pair of lists
\begin{LPNcodedisplay}
[a,woman,shoots,a,man] [].
\end{LPNcodedisplay}
 Think of the first list as
\LPNemph{what needs to be consumed} (or if you prefer:  the \LPNemph{input
list}), and the second list as \LPNemph{what we should leave behind} (or:
the \LPNemph{output list}).  Viewed from this (rather procedural) perspective
the difference list
\begin{LPNcodedisplay}
[a,woman,shoots,a,man] [].
\end{LPNcodedisplay}
 represents the sentence ``a woman shoots a man'' because it says:

 \begin{quote}
 If I consume all the symbols on the left, and leave
        behind the symbols on the right, then I have the sentence
        I am interested in.
 \end{quote}

That is, the sentence we are interested in is the difference
between the contents of these two lists.



That's all we need to know about difference lists to rewrite our
recogniser. If we simply bear in mind the idea of consuming something,
and leaving something behind in mind, we obtain the following
recogniser:
\begin{LPNcodedisplay}
s(X,Z):- np(X,Y), vp(Y,Z).

np(X,Z):- det(X,Y), n(Y,Z).

vp(X,Z):- v(X,Y), np(Y,Z).

vp(X,Z):- v(X,Z).

det([the|W],W).
det([a|W],W).

n([woman|W],W).
n([man|W],W).

v([shoots|W],W).
\end{LPNcodedisplay}

Consider these rules carefully. For example, the ``s" rule says:
\LPNemph{I know that the pair of lists} ``X" \LPNemph{and} ``Z"
\LPNemph{represents a sentence if (1) I can consume} ``X" \LPNemph{and
leave behind a} ``Y", \LPNemph{and the pair} ``X" \LPNemph{and} ``Y"
\LPNemph{represents a noun phrase, and (2) I can then go on to
consume} ``Y" \LPNemph{leaving ``Z'' behind}, \LPNemph{and the pair} ``Y"
"Z" \LPNemph{represents a verb phrase}. The  ``np" rule and the
second of the ``vp" rules work similarly.

Moreover, the same idea underlies the way this grammar handles the
words. For example
\begin{LPNcodedisplay}
n([man|W],W).
\end{LPNcodedisplay}
means we are handling \LPNemph{man} as the difference between
"[man$|$W]" and ``W".  After all, the difference between what is consumed
and what is left behind is precisely the word ``man".

Now, at first this code may be harder to grasp than our previous
recogniser. But note that we have gained something important:
\LPNemph{we haven't used} ``append/3". In the difference list based
recogniser, it simply isn't needed, and this makes a big difference.

How do we use this recogniser? Well, here's how to recognise
sentences:
\begin{LPNcodedisplay}
?- s([a,woman,shoots,a,man],[]).
yes
\end{LPNcodedisplay}
This asks whether we can get an ``s" by consuming the symbols
in ``[a,woman,shoots,a,man]", leaving nothing behind.
Similarly, to generate all the sentences in the grammar,
we ask
\begin{LPNcodedisplay}
?- s(X,[]).
\end{LPNcodedisplay}
This asks: what values can you give to ``X", such that we get an ``s" by
consuming the symbols in ``X", leaving nothing behind?

The queries for other grammatical categories also work the same way.
For example, to find out if ``a woman'' is a noun phrase we ask:
\begin{LPNcodedisplay}
?- np([a,woman],[]).
\end{LPNcodedisplay}
And we generate all the noun phrases in the grammar as follows:
\begin{LPNcodedisplay}
?- np(X,[]).
\end{LPNcodedisplay}


You should trace what happens when this program analyses a sentence
such as \LPNemph{a woman shoots a man}.  As you will see, it is a lot
more efficient than our ``append/3" based program.  Moreover, as no use
is made of ``append/3", the trace is a lot easier to grasp.  So we have
taken a big step forward.

On the other hand, it has to be admitted that the second recogniser is
not as easy to understand, at least at first, and it's a pain having
to keep track of all those difference list variables.  If only it were
possible to have a recogniser as simple as the first and as efficient
as the second. And in fact, it \LPNemph{is} possible: this is  where
DCGs come in.



\section{Definite Clause Grammars}\label{SEC.L7.DCG}

So, what are DCGs?  Quite simply, a nice notation for writing
grammars that hides the underlying difference list variables.
Let's look at three examples.



\subsection*{A first example}\label{SUBSEC.L7.FIRSTEXAMPLE}

As our first example, here's our little grammar written as a DCG:

\begin{LPNcodedisplay}
s --> np,vp.

np --> det,n.

vp --> v,np.
vp --> v.

det --> [the].
det --> [a].

n  --> [woman].
n  --> [man].

v --> [shoots].
\end{LPNcodedisplay}

The link with the original context-free grammar should be transparent:
this is definitely the most user-friendly notation we have used yet.
But how do we use this DCG? In fact, we use it in \LPNemph{exactly}
the same way as we used our difference list recogniser.  For example,
to find out whether \LPNemph{a woman shoots a man} is a sentence, we
pose the query:

\begin{LPNcodedisplay}
?- s([a,woman,shoots,a,man],[]).
\end{LPNcodedisplay}
That is, just as in the difference list recogniser, we ask
whether we can get an ``s" by consuming the symbols in
"[a,woman,shoots,a,man]", leaving nothing behind.

Similarly, to generate all the sentences in the grammar,
we pose the query:
\begin{LPNcodedisplay}
?- s(X,[]).
\end{LPNcodedisplay}
This asks what values we can give to ``X", such that we get an
"s" by consuming the symbols in ``X", leaving nothing behind.

Moreover, the queries for other grammatical categories also work the
same way.  For example, to find out if \LPNemph{a woman} is a noun phrase
we pose the query:
\begin{LPNcodedisplay}
?- np([a,woman],[]).
\end{LPNcodedisplay}
And we generate all the noun phrases in the grammar as follows:
\begin{LPNcodedisplay}
?- np(X,[]).
\end{LPNcodedisplay}


What's going on?  Quite simply, this DCG \LPNemph{is} our difference
list recogniser!  To put it another way, DCG notation is essentially
syntactic sugar, user-friendly notation that lets us write grammars in
a natural way.  But Prolog translates this notation into the kinds of
difference lists discussed before.  So we have the best of both
worlds: a nice simple notation for working with, and the
efficiency of difference lists.

There is an easy way to see what Prolog translates DCG rules into.
Suppose you are working with the DCG just given (that is, suppose that
Prolog has already consulted the rules).  Then if you pose the query:
\begin{LPNcodedisplay}
?- listing(s).
\end{LPNcodedisplay}
you will get the response
\begin{LPNcodedisplay}
s(A,B) :-
    np(A,C),
    vp(C,B).
\end{LPNcodedisplay}
This is what Prolog has translated ``s --$>$ np,vp" into. Note
that (apart from the choice of variables) this is exactly the
difference list rule we used in our second recogniser.

Similarly, if you pose the query
\begin{LPNcodedisplay}
?- listing(np).
\end{LPNcodedisplay}
you will get
\begin{LPNcodedisplay}
np(A,B) :-
    det(A,C),
    n(C,B).
\end{LPNcodedisplay}
This is what Prolog has translated ``np $\rightarrow$ det,n" into. Again
(apart from the choice of variables) this is  the
difference list rule we used in our second recogniser.

To get a complete listing of the translations of all the rules, simply
type
\begin{LPNcodedisplay}
?- listing.
\end{LPNcodedisplay}
There is one thing you may observe. Some Prolog implementations
translate rules such as
\begin{LPNcodedisplay}
det --$>$ [the].
\end{LPNcodedisplay}
not into
\begin{LPNcodedisplay}
det([the|W],W).
\end{LPNcodedisplay}
which was the form we used in our difference list recogniser, but into
\begin{LPNcodedisplay}
det(A,B) :-
    'C'(A,the,B).
\end{LPNcodedisplay}
But although the notation is different, the idea is the same.  This
says you can get a list ``B" from a list ``A" by consuming a ``the".
That is, once again this is a difference list representation.  Note
that ``'C'" is an atom. \index{PROLOG c/3@\texttt{'C'/3}}


\subsection*{Adding recursive rules}\label{SUBSEC.L7.RECURSIVE.RULES}


Our original context-free grammar generated only 20 sentences.
However it is easy to write context-free grammars that generate
infinitely many sentences: simply use recursive rules. Here's
an example. Let's add the following rules to our little grammar:

\begin{center}\begin{tabular}{l}
S $\rightarrow$ S CONJ S\\
CONJ $\rightarrow$ and\\
CONJ $\rightarrow$ or\\
CONJ $\rightarrow$ but
\end{tabular}\end{center}
This rule allows us to join as many sentences together as we like
using the words \LPNemph{and}, \LPNemph{but}, and \LPNemph{or}. So this
grammar classifies sentences such as \LPNemph{The woman shoots the man
or the man shoots the woman} as grammatical.

Now, in principle it is easy to turn this grammar into a DCG. We
need merely add the Prolog rules:

\begin{LPNcodedisplay}
s --> s,conj,s.

conj --> [and].
conj --> [or].
conj --> [but].
\end{LPNcodedisplay}
But there is a problem lurking under the surface. What does Prolog
actually \LPNemph{do} with this DCG? Let's have a look.

First, let's add the new rules at the \LPNemph{beginning} of the
knowledge base, before the rule ``S --$>$ np,vp". What happens if we then
pose the query ``S([a,woman,shoots],[])"? Prolog immediately goes into
a loop.

Can you see why? The point is this. Prolog translates DCG rules into
ordinary Prolog rules. If we place the recursive rule
"s --$>$ s,conj,s" in the knowledge base before the non-recursive
rule
"s --$>$ np,vp" then the knowledge base will contain the following two
Prolog rules, in this order:
\begin{LPNcodedisplay}
s(A, B) :-
        s(A, C),
        conj(C, D),
        s(D, B).

s(A, B) :-
        np(A, C),
        vp(C, B).
\end{LPNcodedisplay}

Now, from a declarative perspective this is fine, but from a
procedural perspective this is fatal.  When it tries to use the first
rule, Prolog immediately encounters the goal ``s(A, C)", which it then
tries to satisfy using the first rule, whereupon it immediately
encounters the goal ``s(A, C)", which it then tries to satisfy using
the first rule, whereupon it immediately encounters the goal ``s(A, C)",
and so on. In short, it goes into an infinite loop and does no
useful work.

So let's add the recursive rule ``s --$>$ s,conj,s" at the end of the
knowledge base, so that Prolog always encounters the translation of
the non-recursive rule first. What happens now, when we pose the query
"s([a,woman,shoots],[])"? Well, now Prolog handles this and gives an
answer. But what happens when we pose the query ``s([woman,shoot],[])"?
Note that this is an ungrammatical sentence that is not accepted by
our grammar. Once again, Prolog gets into an infinite loop. Since it
is impossible to recognise ``[woman,shoot]" as a sentence consisting of
a noun phrase and a verb phrase, Prolog tries to analyse it with the
rule ``s --$>$ s,conj,s", and ends up in the same unending loop as
before.

In short, we are having the same problems that we met when we
discussed recursion, and rule and goal ordering, in
Chapter~\ref{CHAPTER3}.  In a nutshell, ``s --$>$ s,conj,s" translates
into a left-recursive rule, and that's bad news.  Moreover, as we saw
earlier, we \textit{can't} fix such problems simply by tinkering with
the rule ordering: the way out of such difficulties is to change the
goal order of the recursive rule so that the recursive goal is not the
first one in the body of the rule. That is, ideally we should rewrite
the rule so that it is no longer left-recursive.

Nice idea, but unfortunately, it is not an option here.  Why not?
Because the order of the goals determines the order of the words in
the sentence!  It makes an important difference, for example, whether
our grammar accepts \LPNemph{the woman shoots the man and the man
shoots the woman} ("s --$>$ s,conj,s") or whether it accepts
\LPNemph{and the woman shoots the man the man shoots the woman} ("s --$>$ conj,s,s").

But there is a way out.
The standard solution is to introduce a new non-terminal symbol and
rewrite the DCG.  We could, for example, use the category ``simple\_s"
for sentences without embedded sentences. Our grammar would then look
like this:
\begin{LPNcodedisplay}
s --> simple_s,conj,s.
simple_s --> np,vp.
np --> det,n.
vp --> v,np.
vp --> v.
det --> [the].
det --> [a].
n --> [woman].
n --> [man].
v --> [shoots].
conj --> [and].
conj --> [or].
conj --> [but].
\end{LPNcodedisplay}

As you should check, Prolog doesn't get into infinite loops with this
grammar as it did with the previous one, so from a computational
perspective the solution is satisfactory. But it leaves something to
be desired from a linguistic perspective. The DCG that looped was at
least faithful to the linguistic intuitions about the structure of
sentences made using \textit{and}, \textit{but}, and \textit{or}.  The
new DCG imposes an additional layer of structure that is motivated by
processing rather than linguistic considerations; we are no longer
simply turning grammars into Prolog.

The moral is: DCGs aren't magic. They are a nice notation, but you
can't expect to write down an arbitrary CFG as a DCG and have it run
without problems. DCG rules are ordinary Prolog rules in disguise, and
this means that you must pay attention to what your Prolog interpreter
is going to do with them. And in particular, you have to keep an eye
out for left-recursion.


\subsection*{A DCG for a simple formal language}\label{SUBSEC.L7.ANBN}

As our last example, we shall define a DCG for the \LPNterm{formal
language} $a^nb^n$. What is this language? And what is a formal
language anyway?

 A formal language is simply a set of strings. The term ``formal
language'' is intended to contrast with the term ``natural language'':
whereas natural languages are languages that human beings actually
use, formal languages are mathematical objects that computer
scientists, logicians, and mathematicians define and study for various
purposes.

A simple example of a formal language is $a^nb^n$.  The words in this
language are built up from two symbols: the symbol ``a'' and the
symbol ``b''.  In fact, the language $a^nb^n$ consists of all
strings made up from these two symbols that have the following form:
the string must consist of an unbroken block of ``a''s of length
n, followed by an unbroken block of ``b''s of length
n, and nothing else.  So the strings ``ab'', ``aabb'', ``aaabbb'' and ``aaaabbbb' all belong to
$a^nb^n$. (Note that the \LPNterm{empty string} belongs to $a^nb^n$
too: after all, the empty string consists of a block of ``a''s
of length zero followed by a block of ``b''s of length zero.)
On the other hand, ``aba'' and ``abba'' do not belong to
$a^nb^n$.

Now, it is easy to write a context-free grammar that generates this
language:

\begin{center}\begin{tabular}{l}
S $\rightarrow$ $\epsilon$\\
S $\rightarrow$ L S R\\
L $\rightarrow$ a \\
R $\rightarrow$ b
\end{tabular}\end{center}

where $\epsilon$ is the empty string.

The first rule says that an ``s'' can be realised as nothing at
all.  The second rule says that an ``s'' can be made up of an
``l'' (for left) element, followed by an ``s'', followed
by an ``r'' (for right) element. The last two rules say that
``l'' elements and ``r'' elements can be realised as
``a''s and ``b''s respectively. It should be clear that
this grammar really does generate all and only the elements of
$a^nb^n$, including the empty string.

Moreover, it is easy to turn this grammar into DCG. We can do so as
follows:
\begin{LPNcodedisplay}
s --> [].
s --> l,s,r.

l --> [a].
r --> [b].
\end{LPNcodedisplay}
Note that the second rule is recursive (but, thankfully, not left
recursive).  And in fact this DCG works exactly as we would hope. For
example, to the query
\begin{LPNcodedisplay}
?- s([a,a,a,b,b,b],[]).
\end{LPNcodedisplay}
we get the answer ``yes'', while to the query
\begin{LPNcodedisplay}
?- s([a,a,a,b,b,b,b],[]).
\end{LPNcodedisplay}
we get the answer ``no''. The query
\begin{LPNcodedisplay}
?- s(X,[]).
\end{LPNcodedisplay}
enumerates  the strings in the language, starting
from ``[]".


\section{Exercises}\label{SEC.L7.EXERCISES}

\begin{LPNexercise}{L7.EX1}Suppose we are working with the following DCG:
\begin{LPNcodedisplay}
s --> foo,bar,wiggle.
foo --> [choo].
foo --> foo,foo.
bar --> mar,zar.
mar --> me,my.
me --> [i].
my --> [am].
zar --> blar,car.
blar --> [a].
car --> [train].
wiggle --> [toot].
wiggle --> wiggle,wiggle.
\end{LPNcodedisplay}
Write down the ordinary Prolog rules that correspond to these
DCG rules.
What are the first three responses that Prolog gives to the query
``s(X,[])"?
\end{LPNexercise}


\begin{LPNexercise}{L7.EX2}The formal language $a^nb^n - \{\epsilon\}$ consists of all the strings in
$a^nb^n$ except the empty string. Write a DCG that generates this
language.
\end{LPNexercise}


\begin{LPNexercise}{L7.EX3}Let $a^nb^{2n}$ be the formal language which contains all strings of
the following form: an unbroken block of ``a''s of length n
followed by an unbroken block of ``b''s of length 2n, and
nothing else. For example, ``abb'', ``aabbbb'', and ``aaabbbbbb''
belong to $a^nb^{2n}$, and so does the empty string.  Write a DCG that
generates this language.
 \end{LPNexercise}


\section{Practical Session}\label{SEC.L7.PRAXIS}



The purpose of this session is to help you get familiar with DCGs,
difference lists, and the relation between them, and to give you some
experience in writing basic DCGs.  As you will learn in the following
chapter, there is more to DCGs than the ideas just
discussed. Nonetheless, what you have learned so far is certainly the
core, and it is important that you are comfortable with the basic
ideas before moving on.

First some keyboard exercises:


1. Type in or download the simple ``append/3" based recognisers
discussed in the text, and then run some traces.  As you will see, we
were not exaggerating when we said that their performance is
poor. Even for such simple sentences as \LPNemph{The woman shot a man}
you will see that the traces are long and difficult to follow.

2. Next, type in or download our second recogniser, the one based
on difference lists, and run more traces. As you will see, there is a
dramatic gain in efficiency. Moreover, you will see that the traces
are \LPNemph{very} simple to understand, especially when compared with the
monsters produced by the ``append/3" based implementations.

3. Next, type in or download the DCG discussed in the text.  Type
"listing" so that you can see what Prolog translates the rules
to. How does your system translate rules of the form   \index{PROLOG c/3@\texttt{'C'/3}}
"DET $\rightarrow$ [the]"? That is, does it translate them to rules like
"det([the|X],X)", or does is make use of rules
containing the ``'C''' predicate?

4. Now run some traces. Apart from variable names, the traces you
observe here should be very similar to the traces you observed when
running the difference list recogniser.

And now it's time to write some DCGs:


1. The formal language \LPNemph{Even} is very simple: it consists of
all strings containing an even number of ``a''s, and nothing else.
Note that the empty string $\epsilon$ belongs to \LPNemph{Even}.
Write a DCG that generates \LPNemph{Even}.

2. The formal language $a^nb^{2m}c^{2m}d^{n}$ consists of all
strings of the following form: an unbroken block of ``a''s followed
by an unbroken block of ``b''s followed by an unbroken block of
``c''s followed by an unbroken block of ``d''s, such that the ``a''
and ``d'' blocks are exactly the same length, and the ``b'' and
``c'' blocks are also exactly the same length and furthermore consist of
an even number of ``b''s and ``c''s respectively. For example, $\epsilon$,
``abbccd'', and ``aabbbbccccdd all belong to
$a^nb^{2m}c^{2m}d^{n}$.  Write a DCG that generates this
language.

3. The language that logicians call ``propositional logic
over the propositional symbols `p', `q', and `r''' can be defined
by the following context-free grammar:
\begin{center}\begin{tabular}{l}
PROP $\rightarrow$ p \\
PROP $\rightarrow$ q \\
PROP $\rightarrow$ r \\
PROP $\rightarrow$ $\neg$PROP \\
PROP $\rightarrow$ (PROP $\wedge$ PROP) \\
PROP $\rightarrow$ (PROP $\vee$  PROP) \\
PROP $\rightarrow$ (PROP $\rightarrow$ PROP)
\end{tabular}\end{center}

Write a DCG that generates this language. Actually, because we don't
know about Prolog operators yet, you will have to make a few rather
clumsy looking compromises. For example, instead of getting it to
recognise
\begin{center}\begin{tabular}{l}$\neg$(p $\rightarrow$ q)
\end{tabular}\end{center}

you will have to get it recognise things like

\begin{center}\begin{tabular}{l}[not, (, p, implies, q, )]
\end{tabular}\end{center}
instead. We will learn in Chapter~\ref{CHAPTER9} how to deal with
propositional logic somewhat more naturally; in the meantime, write a
DCG that accepts a clumsy looking version of this language. Use
``or'' for $\vee$, and ``and'' for $\wedge$.

