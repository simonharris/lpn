
\chapter{Recursion}\label{CHAPTER3}

\Thischapter{120}{

\noindent
This chapter has two main goals:

\begin{enumerate}
\item{}To introduce recursive definitions in Prolog.


\item{}To show that there can be mismatches between the
      declarative meaning of a Prolog program, and its
      procedural meaning.

\end{enumerate}

}

\section{Recursive Definitions}\label{SEC.L3.RECURSIVE.DEFINITION}



Predicates can be defined recursively.  Roughly speaking, a predicate
is recursively defined if one or more rules in its definition refers
to itself.

\subsection*{Example 1: Eating}\label{SUBSEC.L3.EX1}



Consider the following knowledge base:
\begin{LPNcodedisplay}
is_digesting(X,Y) :- just_ate(X,Y).
is_digesting(X,Y) :-
        just_ate(X,Z),
        is_digesting(Z,Y).

just_ate(mosquito,blood(john)).
just_ate(frog,mosquito).
just_ate(stork,frog).
\end{LPNcodedisplay}


At first glance this seems pretty ordinary: it's just a knowledge base
containing three facts and two rules.  But the definition of the
"is_digesting/2" predicate is \LPNterm{recursive}.  Note that
"is_digesting/2" is (at least partially) defined in terms of itself,
for the "is_digesting/2" functor occurs in both the head and body of
the second rule.  Crucially, however, there is an `escape' from this
circularity.  This is provided by the "just_ate/2" predicate, which
occurs in the first rule.  (Significantly, the body of the first rule
makes no mention of "is_digesting/2".)  Let's now consider both the
\LPNterm{declarative} and \LPNterm{procedural} meanings of this definition.

The word ``declarative'' is used to talk about the logical meaning of
Prolog knowledge bases.  That is, the declarative meaning of a Prolog
knowledge base is simply ``what it says'', or ``what it means, if we read
it as a collection of logical statements''.  And the declarative
meaning of this recursive definition is fairly straightforward.  The
first clause (the escape clause, the one that is not recursive, or
as we shall usually call it, the \LPNterm{base clause}), simply says
that: \LPNemph{if} "X" has just eaten "Y", \LPNemph{then} "X" is now
digesting "Y".  This is obviously a sensible definition.

So what about the second clause, the \LPNterm{recursive clause}?  This
says that: \LPNemph{if} "X" has just eaten "Z"
\LPNemph{and} "Z" is digesting "Y", \LPNemph{then} "X"
is digesting "Y", too. Again, this is obviously a sensible
definition.

So now we know what this recursive definition says, but what happens
when we pose a query that actually needs to use this definition?  That
is, what does this definition actually do?  To use the normal Prolog
terminology, what is its procedural meaning?

This is also reasonably straightforward.  The base rule is like all
the earlier rules we've seen.  That is, if we ask whether "X" is
digesting "Y", Prolog can use this rule to ask instead
the question: has "X" just eaten "Y"?

What about the recursive clause?  This gives Prolog another strategy
for determining whether "X" is digesting "Y":
\LPNemph{it can try to find some} "Z" \LPNemph{such that} "X"
\LPNemph{has just eaten} "Z", and "Z" \LPNemph{is digesting}
"Y".  That is, this rule lets Prolog break the task
apart into two subtasks.  Hopefully, doing so will eventually lead to
simple problems which can be solved by simply looking up the answers
in the knowledge base. The following picture sums up the situation:

\medskip

\begin{center}
\includegraphics{chap3-pspic3.ps}
\end{center}

Let's see how this works. If we pose the query:
\begin{LPNcodedisplay}
?- is_digesting(stork,mosquito).
\end{LPNcodedisplay}
then Prolog goes to work as follows.  First, it tries to make use of
the first rule listed concerning "is_digesting"; that is, the
base rule.  This tells it that "X" is digesting
"Y" if "X" just ate "Y", By unifying
"X" with "stork" and "Y" with
"mosquito" it obtains the following goal:
\begin{LPNcodedisplay}
just_ate(stork,mosquito).
\end{LPNcodedisplay}


But the knowledge base doesn't contain the information that the stork
just ate the mosquito, so this
attempt fails.  So Prolog next tries to make use of the second rule.
By unifying "X" with "stork" and "Y" with
"mosquito" it obtains the following goals:
\begin{LPNcodedisplay}
just_ate(stork,Z),
is_digesting(Z,mosquito).
\end{LPNcodedisplay}
That is, to show
"is_digesting(stork,mosquito)", Prolog
needs to find a value for "Z" such that, firstly,
\begin{LPNcodedisplay}
just_ate(stork,Z).
\end{LPNcodedisplay}
and secondly,
\begin{LPNcodedisplay}
is_digesting(Z,mosquito).
\end{LPNcodedisplay}
And there \LPNemph{is} such a value for "Z", namely
"frog".  It is immediate that
\begin{LPNcodedisplay}
just_ate(stork,frog).
\end{LPNcodedisplay}
will succeed, for this fact is listed in the knowledge base.  And deducing
\begin{LPNcodedisplay}
is_digesting(frog,mosquito).
\end{LPNcodedisplay}
is almost as simple, for the first clause of "is_digesting/2"
reduces this goal to deducing
\begin{LPNcodedisplay}
just_ate(frog,mosquito).
\end{LPNcodedisplay}
and this is a fact listed in the knowledge base.

Well, that's our first example of a recursive rule definition.  We're
going to learn a lot more about them, but one very practical remark
should be made right away.  Hopefully it's clear that when you write a
recursive predicate, it should always have at least two clauses: a
base clause (the clause that stops the recursion at some point), and
one that contains the recursion.  If you don't do this, Prolog can
spiral off into an unending sequence of useless computations.  For
example, here's an extremely simple example of a recursive rule
definition:
\begin{LPNcodedisplay}
p :- p.
\end{LPNcodedisplay}
That's it.  Nothing else.  It's beautiful in its simplicity.  And from
a declarative perspective it's an extremely sensible (if rather
boring) definition: it says ``if property p holds, then property
p holds''. You can't argue with that.

But from a procedural perspective, this is a wildly dangerous rule. In
fact, we have here the ultimate in dangerous recursive rules: exactly
the same thing on both sides, and no base clause to let us escape.
For consider what happens when we pose the following query:
\begin{LPNcodedisplay}
?- p.
\end{LPNcodedisplay}
Prolog asks itself: ``How do I prove "p"?''  and it realises, ``Hey,
I've got a rule for that!  To prove "p" I just need to prove "p"!''.
So it asks itself (again): ``How do I prove "p"?''  and it realises,
``Hey, I've got a rule for that!  To prove "p" I just need to prove
"p"!''.  So it asks itself (yet again): ``How do I prove "p"?''  and
it realises, ``Hey, I've got a rule for that!  To prove "p" I just
need to prove "p"!''  and so on and so forth.

If you make this query, Prolog won't answer you: it will head off,
looping desperately away in an unending search.  That is, it won't
terminate, and you'll have to interrupt it. Of course, if you use
"trace", you can step through one step at a time, until you get
sick of watching Prolog loop.

\subsection*{Example 2: Descendant}\label{SUBSEC.L3.EX2}



Now that we know something about \LPNemph{what} recursion in Prolog
involves, it is time to ask \LPNemph{why} it is so important.  Actually,
this is a question that can be answered on a number of levels, but for
now, let's keep things fairly practical.  So: when it comes to writing
useful Prolog programs, are recursive definitions really so important?
And if so, why?

Let's consider an example.  Suppose we have a knowledge base recording
facts about the child relation:
\begin{LPNcodedisplay}
child(bridget,caroline).
child(caroline,donna).
\end{LPNcodedisplay}
That is, Caroline is a child of Bridget, and Donna is a child of
Caroline. Now suppose we wished to define the descendant relation;
that is, the relation of being a child of, or a child of a child of,
or a child of a child of a child of, and so on.  Here's a first attempt to
do this.  We could add the following two \LPNemph{non}-recursive rules to
the knowledge base:
\begin{LPNcodedisplay}
descend(X,Y) :- child(X,Y).

descend(X,Y) :- child(X,Z),
                 child(Z,Y).
\end{LPNcodedisplay}


Now, fairly obviously these definitions work up to a point, but they are
clearly limited: they only define the concept of
descendant-of for two generations or less.  That's ok for the above
knowledge base, but suppose we get some more information about the
child-of relation and we expand our list of child-of facts to this:
\begin{LPNcodedisplay}
child(anne,bridget).
child(bridget,caroline).
child(caroline,donna).
child(donna,emily).
\end{LPNcodedisplay}

Now our two rules are inadequate.  For example, if we pose the queries
\begin{LPNcodedisplay}
?- descend(anne,donna).
\end{LPNcodedisplay}
or
\begin{LPNcodedisplay}
?- descend(bridget,emily).
\end{LPNcodedisplay}
we get the answer no, which is \LPNemph{not} what we want.  Sure,
we could `fix' this by adding the following two rules:
\begin{LPNcodedisplay}
descend(X,Y) :- child(X,Z_1),
                 child(Z_1,Z_2),
                 child(Z_2,Y).

descend(X,Y) :- child(X,Z_1),
                 child(Z_1,Z_2),
                 child(Z_2,Z_3),
                 child(Z_3,Y).
\end{LPNcodedisplay}

But, let's face it, this is clumsy and hard to read.  Moreover, if we
add further child-of facts, we could easily find ourselves having to
add more and more rules as our list of child-of facts grow, rules
like:
\begin{LPNcodedisplay}
descend(X,Y) :- child(X,Z_1),
                 child(Z_1,Z_2),
                 child(Z_2,Z_3),
                                        .
                     .
                     .
                 child(Z_17,Z_18).
                 child(Z_18,Z_19).
                 child(Z_19,Y).
\end{LPNcodedisplay}
This is not a particularly pleasant (or sensible) way to go!

But we don't need to do this at all. We can avoid
having to use ever longer rules entirely. The
following  recursive predicate definition fixes everything exactly
the way we want:
\begin{LPNcodedisplay}
descend(X,Y) :- child(X,Y).

descend(X,Y) :- child(X,Z),
                 descend(Z,Y).
\end{LPNcodedisplay}



What does this say?  The declarative meaning of the base clause
is: \LPNemph{if} "Y" is a child of "X", \LPNemph{then} "Y" is a
descendant of "X". Obviously sensible.
So what about the recursive clause?  Its declarative meaning is:
\LPNemph{if} "Z" is a child of "X", \LPNemph{and} "Y" is a
descendant of "Z", \LPNemph{then} "Y" is a descendant of
"X". Again, this is obviously true.

So let's now look at the procedural meaning of this recursive
predicate, by stepping through an example. What happens when we pose
the query:
\begin{LPNcodedisplay}
descend(anne,donna)
\end{LPNcodedisplay}
Prolog first tries the first rule. The variable "X" in the head
of the rule is unified with "anne" and "Y" with "donna"  and
the next goal Prolog tries to prove is

\begin{LPNcodedisplay}
child(anne,donna)
\end{LPNcodedisplay}


This attempt fails, however, since the knowledge base
neither contains the fact "child(anne,donna)" nor any rules
that would allow to infer it.
So Prolog backtracks and looks for an alternative way of proving
"descend(anne,donna)". It finds the second rule in the
knowledge base and now has the following subgoals:

\begin{LPNcodedisplay}
child(anne,_633),
descend(_633,donna).
\end{LPNcodedisplay}


Prolog takes the first subgoal and tries to unify  it with something in
the knowledge base. It finds the fact "child(anne,bridget)"
and the variable "_633" gets instantiated to "bridget".
Now that the first subgoal is satisfied, Prolog moves to the
second subgoal. It has to prove

\begin{LPNcodedisplay}
descend(bridget,donna)
\end{LPNcodedisplay}


This is the first recursive call of the predicate "descend/2". As
before, Prolog starts with the first rule, but fails, because the goal

\begin{LPNcodedisplay}
child(bridget,donna)
\end{LPNcodedisplay}
cannot be proved. Backtracking, Prolog finds that there is a second
possibility to be checked for "descend(bridget,donna)", namely the
second rule, which again gives Prolog two new subgoals:

\begin{LPNcodedisplay}
child(bridget,_1785),
descend(_1785,donna).
\end{LPNcodedisplay}


The first one can be unified with the fact
"child(bridget,caroline)" of the knowledge base, so that the
variable "_1785" is instantiated with "caroline". Next Prolog
tries to prove

\begin{LPNcodedisplay}
descend(caroline,donna).
\end{LPNcodedisplay}
This is the second recursive call of predicate "descend/2".
As before, it tries the first rule first, obtaining the following new
goal:

\begin{LPNcodedisplay}
child(caroline,donna)
\end{LPNcodedisplay}


This time Prolog succeeds, since "child(caroline,donna)" is a
fact in the database. Prolog has found a proof for the goal
"descend(caroline,donna)" (the second recursive call). But this
means that "descend(bridget,donna)" (the first recursive call)
is also true, which means that our original query
"descend(anne,donna)" is true as well.

Here is the search tree for the query
"descend(anne,donna)". Make sure that you understand how it
relates to the discussion in the text; that is, how Prolog
traverses this search tree when trying to prove this query.

\begin{center}
\includegraphics{chap3-pspic1.ps}
\end{center}

\bigskip
\bigskip
\bigskip
\medskip

It should be obvious from this example that no matter how many
generations of children we add, we will always be able to work out the
descendant relation. That is, the recursive definition is both general
and compact: it contains \LPNemph{all} the information in the
non-recursive rules, and much more besides.  The non-recursive rules
only defined the descendant concept up to some fixed number of
generations: we would need to write down infinitely many non-recursive
rules if we wanted to capture this concept fully, and of course that's
impossible.  But, in effect, that's what the recursive rule does for
us: it bundles up the information needed to cope with arbitrary
numbers of generations into just three lines of code.

Recursive rules are really important. They enable to pack an enormous
amount of information into a compact form and to define predicates in
a natural way.  Most of the work you will do as a Prolog programmer
will involve writing recursive rules.

\subsection*{Example 3: Successor}\label{SUBSEC.L3.EX3}



In the previous chapter we remarked that building structure through
unification is a key idea in Prolog programming.  Now that we know
about recursion, we can give more interesting illustrations of this.

Nowadays, when human beings write numerals, they usually use
\LPNemph{decimal} notation (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, and
so on) but as you probably know, there are many other notations.  For
example, because computer hardware is generally based on digital
circuits, computers usually use \LPNemph{binary} notation to represent
numerals (0, 1, 10, 11, 100, 101, 110, 111, 1000, and so on), for the
0 can be implemented as a switch being off, the 1 as a switch being
on.  Other cultures use different systems.  For example, the
ancient Babylonians used a base 60 system, while the ancient Romans
used a rather ad-hoc system (I, II, III, IV, V, VI, VII, VIII, IX, X).
This last example shows that notational issues can be important.  If
you don't believe this, try figuring out a systematic way of doing
long-division in Roman notation.  As you'll discover, it's a
frustrating task.  Apparently the Romans had a group of professionals
(analogs of modern accountants) who specialised in this.

Well, here's yet another way of writing numerals, which is
sometimes used in mathematical logic.  It makes use of just four
symbols: 0, \LPNemph{succ}, and the left and right parentheses.  This style
of numeral is defined by the following inductive definition:
\begin{enumerate}
\item{}0 is a numeral.

\item{}If \LPNemph{X} is a numeral, then so is \LPNemph{succ(X)}.
\end{enumerate}


As is probably clear, \LPNemph{succ} can be read as short for
\LPNemph{successor}. That is, \LPNemph{succ(X)} represents the number obtained by
adding one to the number represented by \LPNemph{X}.  So this is a very
simple notation: it simply says that 0 is a numeral, and that all
other numerals are built by stacking \LPNemph{succ} symbols in front.  (In
fact, it's used in mathematical logic because of this simplicity.
Although it wouldn't be pleasant to do household accounts in this
notation, it is a very easy notation to prove things \LPNemph{about}.)

Now, by this stage it should be clear that we can turn this
definition into a Prolog program.  The following knowledge base does
this:
\begin{LPNcodedisplay}
numeral(0).

numeral(succ(X)) :- numeral(X).
\end{LPNcodedisplay}
So if we pose queries like
\begin{LPNcodedisplay}
numeral(succ(succ(succ(0)))).
\end{LPNcodedisplay}
we get the answer yes.

But we can do some more interesting things.  Consider what happens when
we pose the following query:
\begin{LPNcodedisplay}
numeral(X).
\end{LPNcodedisplay}
That is, we're saying ``Ok, show me some numerals''.  Then we can have
the following dialogue with Prolog:
\begin{LPNcodedisplay}
X = 0 ;

X = succ(0) ;

X = succ(succ(0)) ;

X = succ(succ(succ(0))) ;

X = succ(succ(succ(succ(0)))) ;

X = succ(succ(succ(succ(succ(0))))) ;

X = succ(succ(succ(succ(succ(succ(0)))))) ;

X = succ(succ(succ(succ(succ(succ(succ(0))))))) ;

X = succ(succ(succ(succ(succ(succ(succ(succ(0))))))))
yes
\end{LPNcodedisplay}


Yes, Prolog is counting: but what's really important is \LPNemph{how} it's
doing this. Quite simply, it's backtracking through the recursive
definition, and actually \LPNemph{building} numerals using unification. This
is an instructive example, and it is important that you understand
it. The best way to do so is to sit down and try it out, with
"trace" turned on.

Building and binding.  Recursion, unification, and proof search.  These
are ideas that lie at the heart of Prolog programming.  Whenever we
have to generate or analyse recursively structured objects (such as
these numerals) the interplay of these ideas makes Prolog a powerful
tool.  For example, in the next chapter we shall introduce
\LPNterm{lists}, an extremely important recursive data structure, and
we will see that Prolog is a natural list processing language.  Many
applications (computational linguistics is a prime example) make heavy
use of recursively structured objects, such as trees and feature
structures.  So it's not particularly surprising that Prolog has
proved useful in such applications.

\subsection*{Example 4: Addition}\label{SUBSEC.L3.EX4}



As a final example, let's see whether we can use the representation of
numerals that we introduced in the previous section for doing simple
arithmetic. Let's try to define addition. That is, we want to define a
predicate "add/3" which when given two numerals as the
first and second argument returns the result of adding them up as its
third argument. For example:
\begin{LPNcodedisplay}
?- add(succ(succ(0)),succ(succ(0)),
       succ(succ(succ(succ(0))))).
yes
?- add(succ(succ(0)),succ(0),Y).
Y = succ(succ(succ(0)))
\end{LPNcodedisplay}

There are two things which are important to notice:
\begin{enumerate}
\item{}Whenever the first argument is "0", the third argument has
to be the same as the second argument:
\begin{LPNcodedisplay}
?- add(0,succ(succ(0)),Y).
Y = succ(succ(0))
?- add(0,0,Y).
Y = 0
\end{LPNcodedisplay}

This is the case that we want to use for the base clause.
\item{}Assume that we want to add the two numerals "X" and "Y"
(for example  "succ(succ(succ(0)))" and "succ(succ(0))") and that
"X" is not "0". Now, if
"X1" is the numeral that has one "succ" functor less than
"X" (that is, "succ(succ(0))" in our example) and if we know the
result -- let's call it "Z" -- of adding "X1" and "Y"
(namely "succ(succ(succ(succ(0))))"), then it is very easy to
compute the result of adding "X" and "Y": we just have to
add one "succ"-functor to "Z". This is what we want to
express with the recursive clause.
\end{enumerate}

Here is the predicate definition that expresses exactly what we just said:
\begin{LPNcodedisplay}
add(0,Y,Y).
add(succ(X),Y,succ(Z)) :-
        add(X,Y,Z).
\end{LPNcodedisplay}

So what happens, if we give Prolog this predicate definition and then
ask:
\begin{LPNcodedisplay}
?- add(succ(succ(succ(0))), succ(succ(0)), R).
\end{LPNcodedisplay}
 Let's go step by step through the way Prolog processes this
query. The trace and search tree for the query are given below.

The first argument is not "0", which means that only the second clause
for "add/3" can be used. This leads to a recursive call of "add/3". The
outermost "succ" functor is stripped off the first argument of the
original query, and the result becomes the first argument of the
recursive query. The second argument is passed on unchanged to the
recursive query, and the third argument of the recursive query is a
variable, the internal variable "_G648" in the trace given below. Note
that "_G648" is not instantiated yet. However it shares values with
"R" (the variable that we used as the third argument in the original
query) because "R" was instantiated to "succ(_G648)" when the query
was unified with the head of the second clause. But that means that
"R" is not a completely uninstantiated variable anymore. It is now a
complex term, that has a (uninstantiated) variable as its argument.

The next two steps are essentially the same. With every step the first
argument becomes one layer of "succ" smaller; both the trace and the
search tree given below show this nicely.  At the same time, a "succ"
functor is added to "R" at every step, but always leaving
the innermost variable uninstantiated. After the first recursive
call "R" is "succ(_G648)".  After the second recursive call, "_G648"
is instantiated with "succ(_G650)", so that "R" is "succ(succ(_G650)".
After the third recursive call, "_G650" is instantiated with
"succ(_G652)" and "R" therefore becomes "succ(succ(succ(_G652)))". The
search tree shows this step by step instantiation.

At this stage all "succ" functors have been stripped off the first
argument and we can apply the base clause. The third argument is
equated with the second argument, so the  `hole' (the
uninstantiated variable) in the complex term "R" is finally filled,
and we are through.

Here's the complete trace of our query:
\begin{LPNcodedisplay}
Call: (6) add(succ(succ(succ(0))), succ(succ(0)), R)

Call: (7) add(succ(succ(0)), succ(succ(0)), _G648)

Call: (8) add(succ(0), succ(succ(0)), _G650)

Call: (9) add(0, succ(succ(0)), _G652)

Exit: (9) add(0, succ(succ(0)), succ(succ(0)))

Exit: (8) add(succ(0), succ(succ(0)), succ(succ(succ(0))))

Exit: (7) add(succ(succ(0)), succ(succ(0)),
                                succ(succ(succ(succ(0)))))

Exit: (6) add(succ(succ(succ(0))), succ(succ(0)),
                          succ(succ(succ(succ(succ(0))))))
\end{LPNcodedisplay}

And here's the search tree:

\begin{center}
\includegraphics{chap3-pspic2.ps}
\end{center}

\section{Rule Ordering, Goal Ordering, and Termination}\label{SEC.L3.RO.CO.TERM}



Prolog was the first reasonably successful attempt to create a
\LPNterm{logic programming} language.  Underlying logic programming is
a simple (and seductive) vision: the task of the programmer is simply
to \LPNemph{describe} problems.  The programmer should write down (in
the language of logic) a declarative specification (that is: a
knowledge base), which describes the situation of interest.  The
programmer shouldn't have to tell the computer \LPNemph{what} to do.
To get information, he or she simply asks the questions.  It's up to
the logic programming system to figure out how to get the answer.

Well, that's the idea, and it should be clear that Prolog has taken
some important steps in this direction.  But Prolog is \LPNemph{not},
repeat \LPNemph{not}, a full logic programming language.  If you only
think about the declarative meaning of a Prolog program, you are in
for a very tough time.  As we learned in the previous chapter, Prolog
has a very specific way of working out the answers to queries: it
searches the knowledge base from top to bottom, clauses from left to
right, and uses backtracking to recover from bad choices. These
procedural aspects have an important influence on what actually
happens when you make a query. We have already seen a dramatic example
of a mismatch between the procedural and declarative meaning of a
knowledge base (remember the "p:- p" program?), and as we shall now
see, it is easy to define knowledge bases which (read logically)
describe the same situations, but which behave very differently.
Let's consider the matter.

Recall our earlier descendant program (let's call it "descend1.pl"):
\begin{LPNcodedisplay}
child(anne,bridget).
child(bridget,caroline).
child(caroline,donna).
child(donna,emily).

descend(X,Y) :- child(X,Y).

descend(X,Y) :- child(X,Z),
                 descend(Z,Y).
\end{LPNcodedisplay}


We'll make one change to it, and call the result "descend2.pl":
\begin{LPNcodedisplay}
child(anne,bridget).
child(bridget,caroline).
child(caroline,donna).
child(donna,emily).

descend(X,Y) :- child(X,Z),
                 descend(Z,Y).

descend(X,Y) :- child(X,Y).
\end{LPNcodedisplay}

All we have done is change the \LPNterm{rule order}. So if we read the
program as a purely logical definition, nothing has changed.  But does
the change give rise to procedural differences?  Yes, but nothing
significant. For example, if you work through the examples you
will see that the first solution that "descend1.pl" finds is
\begin{LPNcodedisplay}
X = anne
Y = bridget
\end{LPNcodedisplay}
whereas the first solution that "descend2.pl" finds is
\begin{LPNcodedisplay}
X = anne
Y = emily
\end{LPNcodedisplay}
But (as you should check) both programs generate exactly the same
answers, they merely find them in a different order. And this is a
general point. Roughly speaking (we'll add a caveat later on) changing
the order of rules in a Prolog program does not change (up to the
order in which solutions are found) the program's behaviour.

So let's move on. We'll make one small change to "descend2.pl", and
call the result "descend3.pl":
\begin{LPNcodedisplay}
child(anne,bridget).
child(bridget,caroline).
child(caroline,donna).
child(donna,emily).

descend(X,Y) :- descend(Z,Y),
                 child(X,Z).

descend(X,Y) :- child(X,Y).
\end{LPNcodedisplay}

Note the difference. Here we've changed the \LPNterm{goal order}
\LPNemph{within}
a rule, not the rule order.  Now, once again, if we read the
program as a purely logical definition, nothing has changed; it means
the same thing as the previous two versions. But this time the
program's behaviour has changed dramatically.  For example, if you
pose the query
\begin{LPNcodedisplay}
descend(anne,emily).
\end{LPNcodedisplay}
you will get an error message (``out of local stack'', or something
similar). Prolog is looping. Why?  Well, in order to satisfy the query
"descend(anne,emily)" Prolog uses the first rule. This means that
its next goal will be to satisfy the query
\begin{LPNcodedisplay}
descend(W1,emily)
\end{LPNcodedisplay}
for some new variable "W1". But to satisfy this new goal, Prolog
again has to use the first rule, and this means that its next goal is
going to be
\begin{LPNcodedisplay}
descend(W2,emily)
\end{LPNcodedisplay}
for some new variable "W2". And of course, this in turn means that its
next goal is going to be "descend(W3,emily)" and then
"descend(W4,emily)", and so on.  That is, the (at first glance
innocuous) change in the goal order has resulted in procedural
disaster.  To use the standard terminology, we have here a classic
example of a \LPNterm{left recursive rule}, that is, a rule where the
leftmost item of the body is identical (modulo the choice of
variables) with the rule's head.  As our example shows, such rules
easily give rise to non-terminating computations.  Goal order, and in
particular left recursion, is the root of all evil when it comes to
non-termination.

Still, as we said earlier, we need to make one small caveat about rule
ordering. We said earlier that rule ordering only changes the order in
which solutions are found. However this may not be true
if we are working with non-terminating programs. To see this, consider
the fourth (and last) variant of our descendant program, namely
"descend4.pl":
\begin{LPNcodedisplay}
child(anne,bridget).
child(bridget,caroline).
child(caroline,donna).
child(donna,emily).

descend(X,Y) :- child(X,Y).

descend(X,Y) :- descend(Z,Y),
                 child(X,Z).
\end{LPNcodedisplay}
This program is "descend3.pl" with the rule ordering
reversed.  Now (once again) this program has the same declarative
meaning as the other variants, but it is also procedurally different from
its relatives. First, and most obviously, it is very different
procedurally from both "descend1.pl" and "descend2.pl".  In
particular, because it contains a left recursive rule, this new
program does not terminate on some input.  For example (just like
"descend3.pl") this new program does not terminate when we pose the
query
\begin{LPNcodedisplay}
descend(anne,emily).
\end{LPNcodedisplay}
But "descend4.pl" is not procedurally identical to "descend3.pl".
The rule ordering reversal does make a difference. For example,
"descend3.pl" will not terminate if we pose the query
\begin{LPNcodedisplay}
descend(anne,bridget).
\end{LPNcodedisplay}
However "descend4.pl" will terminate in this case, for the rule
reversal enables it to apply the non-recursive rule and halt. So when
it comes to non-terminating programs, rule ordering changes can lead
to some extra solutions being found. Nonetheless, goal ordering, not
rule ordering, is what is truly procedurally significant.  To ensure
termination, we need to pay attention to the order of goals within the
bodies of rules.  Tinkering with rule orderings does not  get to
grips with the roots of termination problems --- at best it can yield
some extra solutions.

Summing up, our four variant descendant programs are Prolog knowledge
bases which describe exactly the same situations, but behave
differently.  The difference in behaviour between "descend1.pl" and
"descend2.pl" (which differ only in the way rules are ordered) is
relatively minor: they generate the same solutions, but in a different
order.  But "descend3.pl" and "descend4.pl" are procedurally very
different from their two cousins, and this is because they differ from
them in the way their goals are ordered.  In particular, both these
variants contain left recursive rules, and in both cases this leads to
non-terminating behaviour.  The change in rule ordering
between "descend3.pl" and "descend4.pl" merely means that
"descend4.pl" will terminate in some cases where "descend3.pl" will
not.


What are the ramifications of our discussion for the practicalities of
producing working Prolog programs? It's probably best to say the following.
Often you can get the overall idea (the big picture) of how to write the
program by thinking declaratively, that is, by thinking in terms of describing
the problem accurately.  This is an excellent way to approach problems, and
certainly the one most in keeping with the spirit of logic programming.  But
once you've done that, you need to think about how Prolog will work with
knowledge bases you have written.  In particular, to ensure termination, you
need to check that the goal orderings you have given are sensible.  The basic
rule of thumb is never to write as the leftmost goal of the body something
that is identical (modulo variable names) with the goal given in the head.
Rather, place such goals (which trigger recursive calls) as far as possible
towards the right of the tail. That is, place them after the goals which test
for the various (non-recursive) termination conditions. Doing this gives
Prolog a sporting chance of fighting it's way through your recursive
definitions to find solutions.


\clearpage
\section{Exercises}\label{SEC.L3.EXERCISES}


\begin{LPNexercise}{L3.EX5}In the text, we discussed the predicate
\begin{LPNcodedisplay}
descend(X,Y) :- child(X,Y).
descend(X,Y) :- child(X,Z),
                 descend(Z,Y).
\end{LPNcodedisplay}
Suppose we reformulated this predicate as follows:
\begin{LPNcodedisplay}
descend(X,Y) :- child(X,Y).
descend(X,Y) :- descend(X,Z),
                 descend(Z,Y).
\end{LPNcodedisplay}
\end{LPNexercise}
Would this
be problematic?

\begin{LPNexercise}{L3.EX1}Do you know these wooden Russian dolls
(Matryoshka dolls) where the smaller ones are contained in bigger
ones?  Here is a schematic picture:

\includegraphics[width=4cm]{dolls.eps}

First, write a knowledge base using the predicate "directlyIn/2" which
encodes which doll is directly contained in which other doll.  Then,
define a recursive predicate "in/2", that tells us which doll is
(directly or indirectly) contained in which other dolls. For example,
the query "in(katarina,natasha)" should evaluate to true, while
"in(olga, katarina)" should fail.
\end{LPNexercise}


\begin{LPNexercise}{L3.EX3}We have the following knowledge base:
\begin{LPNcodedisplay}
directTrain(saarbruecken,dudweiler).
directTrain(forbach,saarbruecken).
directTrain(freyming,forbach).
directTrain(stAvold,freyming).
directTrain(fahlquemont,stAvold).
directTrain(metz,fahlquemont).
directTrain(nancy,metz).
\end{LPNcodedisplay}

That is, this knowledge base holds facts about towns it is possible to
travel between by taking a \LPNemph{direct} train.  But of course, we can
travel further by chaining together direct train journeys.  Write a
recursive predicate "travelFromTo/2" that tells us when we can
travel by train between two towns.  For example, when given the query
\begin{LPNcodedisplay}
travelFromTo(nancy,saarbruecken).
\end{LPNcodedisplay}
it should reply yes.
\end{LPNexercise}


\begin{LPNexercise}{L3.EX2}Define a predicate "greater_than/2"
that takes two numerals in the notation that we introduced in the text
(that is, 0, succ(0), succ(succ(0)), and so on) as arguments and
decides whether the first one is greater than the second one. For example:
\begin{LPNcodedisplay}
?- greater_than(succ(succ(succ(0))),succ(0)).
yes
?- greater_than(succ(succ(0)),succ(succ(succ(0)))).
no
\end{LPNcodedisplay}
\end{LPNexercise}
\begin{LPNexercise}{L3.EX4}Binary trees
are trees where all internal nodes have exactly two children. The
smallest binary trees consist of only one leaf node. We will represent
leaf nodes as "leaf(Label)". For instance, "leaf(3)" and "leaf(7)" are
leaf nodes, and therefore small binary trees. Given two binary trees
"B1" and "B2" we can combine them into one binary tree using the
functor "tree/2" as follows: "tree(B1,B2)".  So, from the leaves
"leaf(1)" and "leaf(2)" we can build the binary tree
"tree(leaf(1),leaf(2))". And from the binary trees
"tree(leaf(1),leaf(2))" and "leaf(4)" we can build the binary tree
"tree(tree(leaf(1), leaf(2)),leaf(4))".


Now, define a predicate "swap/2", which produces the mirror
image of the binary tree that is its first argument. For example:
\begin{LPNcodedisplay}
?- swap(tree(tree(leaf(1), leaf(2)), leaf(4)),T).
T = tree(leaf(4), tree(leaf(2), leaf(1))).
yes
\end{LPNcodedisplay}
\end{LPNexercise}




\section{Practical Session}\label{SEC.L3.PRAXIS}



By now, you should feel more at home with writing and running basic
Prolog programs. In this practical session we first suggest two series
of keyboard exercises which will help you get familiar with recursive
definitions in Prolog, and then give you some programming problems to
solve.

First the keyboard exercises. As recursive programming is so
fundamental to Prolog, it is important that you have a firm grasp of
what it involves. In particular, it is important that you understand
the process of variable instantiation when recursive definitions are
used, and that you understand why the order of goals in rules can make
the difference between termination and non-termination. So:

\begin{enumerate}


\item{} Load "descend1.pl", turn on "trace", and pose the query
"descend(anne,emily)". Count how many steps it takes Prolog to work
out the answer (that is, how many times do you have to hit the return
key).  Now turn "trace" off and pose the query "descend(X,Y)". How
many answers are there?

\item{}Load "descend2.pl". This is the variant of "descend1.pl" with
the rule order reversed. Repeat the traces you have
carried out for "descend1.pl", and compare the results.

\item{}Load "descend3.pl". This is the variant of "descend2.pl" in
which the goal order within the recursive rule is switched, resulting
in a left recursive rule.  Because of this, even for such simple
queries as "descend(anne,bridget)", Prolog will not terminate.  Step
through an example, using "trace", to confirm this.

\item{}Load "descend4.pl". This is the variant of "descend3.pl"
obtained by switching the rule order.  So "descend4.pl" also contains
a left recursive rule, and does not terminate on all input.  But it
does terminate on some input where "descend3.pl" does not.  Which
extra solutions does it find?

\end{enumerate}

As we said in the text, goal ordering, not rule ordering is
what is truly  procedurally significant.  But with non-terminating programs,
rule ordering changes can have unexpected effects.  Recall the
successor program discussed in the text (let's call it "numeral1.pl"):
\begin{LPNcodedisplay}
numeral(0).
numeral(succ(X)) :- numeral(X).
\end{LPNcodedisplay}
Let's  swap the order of the two clauses, and
call the result "numeral2.pl":
\begin{LPNcodedisplay}
numeral(succ(X)) :- numeral(X).
numeral(0).
\end{LPNcodedisplay}
Clearly the declarative, or logical, content of this program is
exactly the same as the earlier version. But what are the procedural
differences, if any?

\begin{enumerate}

\item{} Create a file containing "numeral2.pl", load it, and
investigate what happens if we pose  queries about
\LPNemph{specific} numerals.
For example, suppose we ask:
\begin{LPNcodedisplay}
numeral(succ(succ(succ(0)))).
\end{LPNcodedisplay}
Do "numeral1.pl" and "numeral2.pl" behave in the same way on such
input?

\item{}
Second, look at what happens if we try to \LPNemph{generate} numerals, that
is, suppose we pose the query
\begin{LPNcodedisplay}
numeral(X).
\end{LPNcodedisplay}
Do the programs display identical behaviour?
\end{enumerate}

Here are  some programs for you to try your hand at.
\begin{enumerate}
\item{}Imagine that the following knowledge base describes a maze. The facts
determine which points are connected, that is, from which points you can
get to which other points in one step. Furthermore, imagine that all
paths are one-way streets, so that you can only walk them in one
direction. So, you can get from point 1 to point 2, but not the other
way round.
\begin{LPNcodedisplay}
connected(1,2).
connected(3,4).
connected(5,6).
connected(7,8).
connected(9,10).
connected(12,13).
connected(13,14).
connected(15,16).
connected(17,18).
connected(19,20).
connected(4,1).
connected(6,3).
connected(4,7).
connected(6,11).
connected(14,9).
connected(11,15).
connected(16,12).
connected(14,17).
connected(16,19).
\end{LPNcodedisplay}

Write a predicate "path/2" that tells you from which points
in the maze you can get to which other points when chaining together
connections given in the above knowledge base. Can you get from point
5 to point 10? Which other point can you get to when starting at point
1? And which points can be reached from point 13?
\item{}We are given the following knowledge base of travel information:
\begin{LPNcodedisplay}
byCar(auckland,hamilton).
byCar(hamilton,raglan).
byCar(valmont,saarbruecken).
byCar(valmont,metz).

byTrain(metz,frankfurt).
byTrain(saarbruecken,frankfurt).
byTrain(metz,paris).
byTrain(saarbruecken,paris).

byPlane(frankfurt,bangkok).
byPlane(frankfurt,singapore).
byPlane(paris,losAngeles).
byPlane(bangkok,auckland).
byPlane(singapore,auckland).
byPlane(losAngeles,auckland).
\end{LPNcodedisplay}

Write a predicate "travel/2" which determines whether it is
possible to travel from one place to another by chaining together
car, train, and plane journeys. For example, your program should answer
yes to the query "travel(valmont,raglan)".

\item{}So, by using "travel/2" to query the above database, you can
find out that it is possible to go from Valmont to Raglan.  If you are
planning such a voyage, that's already something useful to know, but
you would probably prefer to have the precise route from Valmont to
Raglan.  Write a predicate "travel/3" which tells you which route to
take when travelling from one place to another.  For example, the
program should respond
\begin{LPNcodedisplay}
X = go(valmont,metz,
       go(metz,paris,
          go(paris,losAngeles)))
\end{LPNcodedisplay}
to the query
"travel(valmont,losAngeles,X)".

\item{}Extend the predicate "travel/3" so that it not only tells you
the route to take to get from one place to another, but also
\LPNemph{how} you have to travel. That is, the new program should let
us know, for each stage of the voyage, whether we need to travel by
car, train, or plane.  \end{enumerate}
